{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945f9adf-0027-4add-825a-21b520b80d35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a3ec7-2bb6-4df1-9b14-0a8fb818d77e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Tidy Data from Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb51395-1700-4967-ba54-a9235bbac053",
   "metadata": {},
   "source": [
    "An Excel spreadsheet with some brief information on awards given to movies is available at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/Film_Awards.xlsx\n",
    "\n",
    "In a more fleshed out case, we might have data for many more years, more types of awards, more associations that grant awards, and so on.  While the organization of this spreadsheet is much like a great many you will encounter \"in the wild,\" it is very little like the tidy data we would rather work with.  In the simple example, only 63 data values occur, and you could probably enter them into the desired structure by hand as quickly as coding the transformations.  However, the point of this exercise is to write programming code that could generalize to larger data sets of similar structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a7b14-1755-4996-815a-ccc52f5ca3fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"img/Film_Awards.png\" alt=\"Film Awards\"/>\n",
    "\n",
    "__Image: Film Awards Spreadsheet__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a9a8a-37a6-40c8-a448-c61d6eb5bc5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Your task in this exercise is to read this data into a single well-normalized data frame, using whichever language and library you are most comfortable with.  Along the way, you will need to remediate whatever data integrity problems you detect.  As examples of issues to look out for:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21690088-d166-44cf-b635-b7874a48f1b3",
   "metadata": {},
   "source": [
    "* The film _1917_ was stored as a number not a string when naïvely entered into a cell.\n",
    "* The spelling of some values is inconsistent.  Olivia Colman's name is incorrectly transcribed as 'Coleman' in one occurrence.  There is a spacing issue in one value you will need to identify.\n",
    "* Structurally, an apparent parallel is not really so.  Person names are sometimes listed under the name of the association, but elsewhere under another column.  Film names are sometimes listed under association, other times elsewhere.\n",
    "* Some column names occur multiple times in the same tabular area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6bcf7-a605-45f7-9c24-d85f6a84c593",
   "metadata": {},
   "source": [
    "In thinking about a good data frame organization, think of what the independent and dependent variables are.  In each year, each association awards for each category. These are independent dimensions.  A person name and a film name are slightly tricky since they are not exactly independent, but at the same time some awards are to a film and others to a person.  Moreover, one actor might appear in multiple films in a year (not in this sample data; but do not rule it out).  Likewise, at times multiple films have used the same name at times in film history. Some persons are both director and actor (in either the same or different films)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e28898-2820-4688-b565-ceec459672e0",
   "metadata": {},
   "source": [
    "Once you have a useful data frame, use it to answer these questions in summary reports:\n",
    "\n",
    "* For each film involved in multiple awards, list the award and year it is associated with.\n",
    "* For each actor/actress winning multiple awards, list the film and award they are associated with.\n",
    "* While not occurring in this small data set, sometimes actors/actresses win awards for multiple films (usually in different years).  Make sure your code will handle that situation.\n",
    "* It is manual work, but you may want to research and add awards given in other years; in particular, adding some data will show actors with awards for multiple films.  Do your other reports correctly summarize the larger data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360c680-2a0f-42b9-a873-7b9ec7156648",
   "metadata": {},
   "source": [
    "### Tidy Data from SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db58cd-080f-4bee-949a-54f87484c2c7",
   "metadata": {},
   "source": [
    "An SQLite database with roughly the same brief information as in the prior spreadsheet is available at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/Film_Awards.sqlite\n",
    "\n",
    "However, the information in the database version is relatively well normalized and typed.  Also, additional information has been included on a variety of entities included in the spreadsheet.  Only slightly more information is included in this schema than in the spreadsheet, but it should be able to accommodate a large amount of data on films, actors, directors, and awards, and the relationships among those data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d014f-0a24-4914-8e5d-5e30c5c20e41",
   "metadata": {},
   "source": [
    "```sql\n",
    "sqlite> .tables\n",
    "actor     award     director  org_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c497f33-235f-46dc-b217-66d4a1082aa8",
   "metadata": {},
   "source": [
    "As was mentioned in the prior exercise, the same name for a film can be used more than once, even by the same director.  For example  Abel Gance, used the title _J'accuse!_ for both his 1919 and 1938 films with connected subject matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36568c3-f289-42e0-aa12-96b81f8fa4b9",
   "metadata": {},
   "source": [
    "```\n",
    "sqlite> SELECT * FROM director WHERE year < 1950;\n",
    "Abel Gance|J'accuse!|1919\n",
    "Abel Gance|J'accuse!|1938\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84835ef-3cfe-48a0-9231-42c4df80db8e",
   "metadata": {},
   "source": [
    "Let us look at a selection from the `actor` table, for example.  In this table we have a column `gender` to differentiate beyond name. As of this writing, no transgender actor has been nominated for a major award both before and after a change in gender identity, but this schema allows for that possibility.  In any case, we can use this field to differentiate the \"actor\" versus \"actress\" awards that many organizations grant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699497c-20a4-4534-9f8d-4cc8b95c5e63",
   "metadata": {},
   "source": [
    "```sql\n",
    "sqlite> .schema actor\n",
    "CREATE TABLE actor (name TEXT, film TEXT, year INTEGER, gender CHAR(1));\n",
    "\n",
    "sqlite> SELECT * FROM actor WHERE name=\"Joaquin Phoenix\";\n",
    "Joaquin Phoenix|Joker|2019|M\n",
    "Joaquin Phoenix|Walk the Line|2006|M\n",
    "Joaquin Phoenix|Hotel Rwanda|2004|M\n",
    "Joaquin Phoenix|Her|2013|M\n",
    "Joaquin Phoenix|The Master|2013|M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb4d9c-ffd5-480b-938f-f3def00d15b8",
   "metadata": {},
   "source": [
    "The goal in this exercise is to create the same tidy data frame that you created in the prior exercise, and answer the same questions that were asked there.  If some questions can be answered directly with SQL, feel free to take that approach instead.  For this exercise, only consider awards for the years 2017, 2018, and 2019.  Some others are included in an incomplete way, but your reports are for those years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7040ef9-b557-457c-8a79-46f1bf8519a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```sql\n",
    "sqlite> SELECT * FROM award WHERE winner=\"Frances McDormand\";\n",
    "Oscar|Best Actress|2017|Frances McDormand\n",
    "GG|Actress/Drama|2017|Frances McDormand\n",
    "Oscar|Best Actress|1997|Frances McDormand\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233bbb9-d70a-41c9-b789-eac8a1d46cc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hierarchical Data\n",
    "\n",
    "The two exercises here deal first with refining the processing of the geographic data that is available in several formats.  The second exercise addresses moving between a key/value and relational model for data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb863c26-671e-471f-850f-cff949a719f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exploring Filled Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933eadf-9811-4076-8087-cf1d17821a88",
   "metadata": {},
   "source": [
    "Using the United States county data we created tidy data frames that contained the extents of counties as simple cardinal direction limits; we also were provided with the \"census area\" of each county.  Unfortunately, the data available here does not specifically address water bodies and their sizes, which might occur within counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec04062-5449-4b31-821b-70bc1fe45f1e",
   "metadata": {},
   "source": [
    "The census data can be found at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.json\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.kml\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce83fad-e78e-41ea-9d4a-82b78d6e9caf",
   "metadata": {},
   "source": [
    "In this exercise you will create an additional column in the data frame illustrated in the text to hold the percentage of the \"bounding box\" of a county that is occupied by the census area.  The trick, of course, is that the surface area enclosed by latitude/longitude corners, is not a simple rectangle, nor even a trapezoid, but rather a portion of a spherical surface.  County shapes themselves are typically not rectangular, and may include discontiguous regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2d4ce-288b-4b95-99fa-ba537cbab59a",
   "metadata": {},
   "source": [
    "To complete this exercise, you may either reason mathematically about this area (the simplifying assumption that the Earth is a sphere is acceptable) or identify appropriate GIS software to do this calculation for you.  The result of your work will be a data frame like that presented in the chapter, but with a column called `\"occupied\"` that contains 3221 floating point values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b75641-fe2b-47d1-8497-a81890b8d879",
   "metadata": {},
   "source": [
    "For extra credit you can investigate or improve a few additional data integrity issues.  The Shapefile in the zip archive is the canonical data provided by the US Census Bureau.  The code we saw in this chapter to process GeoJSON and KML actually produce slightly different results for latitude/longitude locations, at the third decimal place.  Presumably, the independent developer whom I downloaded these conversions from allowed some data error to creep in somehow.  Diagnose which version, if either, matches the original `.shp` file, and try to characterize the reason for and degree of the discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfede5b-4f5d-40dd-b44a-b24e047335ad",
   "metadata": {},
   "source": [
    "For additional extra credit, fix the `kml_county_summary()` function presented in this chapter so that it correctly handles `<MultiGeometry>` county shapes rater than skipping over them.  How often did this problem occur among the 3221 United States counties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63a3fe-1659-4825-bfba-265ee5cb4683",
   "metadata": {},
   "source": [
    "### Create a Relational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146e8db-179b-4e4b-8c41-286b99c927dd",
   "metadata": {},
   "source": [
    "The key/value data in the DBM restaurant data is organized in a manner that might provide very fast access in Redis or similar systems.  But there is certainly a mismatch with the implicit data model.  Keys have structure in their hierarchy, but it is a finite and shallow hierarchy.  Values may be of several different implicit data types; in particular, ratings are stored as strings, but they really represent sequences of small integer values.  Other fields are simple strings (albeit stored as bytes in the DBM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb14795-b07e-4923-a8cb-0043829ce097",
   "metadata": {},
   "source": [
    "The `dbm` module in the shown example uses Python's fallback \"dumb DBM\" format which does not depend on external drivers like GDBM or LDBM.  For the example with hundreds of records this is quite fast; if you wished to used millions of records, other systems would scale well and are preferred.  This \"dumb\" format actually consistes of three separate files, but sharing the `keyval.db` prefix; the three are provided as a zip archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724cda1-dbdd-4a46-93e6-71dfa0b19cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbm.whichdb('data/keyval.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9dc6f-242a-45d4-a23a-1a77598fcbf1",
   "metadata": {},
   "source": [
    "The \"dbm.dumb\" format is not necessarily portable to other programming languages.  It is, however, simple enough that you could write an adapter rather easily.  To provide the identical data in a more universal format, a CSV of the identical content is also available:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/keyval.zip\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/keyval.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a08a0-44a8-46ef-9890-70cbb4891746",
   "metadata": {},
   "source": [
    "For this assignment you should transform the key/value data in this example into relational tables, using foreign keys where appropriate, and making good decisions about data types.  SQLite is an excellent choice for a database system to target; it is discussed in chapter 1 (*Data Ingesion – Tabular Formats*).  Any other RDBMS is also a good choice if you have administrative access (i.e. table creation rights).  Before transforming the data model, you will need to clean up the inconsistencies in the hierarchical keys that were discussed in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c574e-468f-4b19-a54f-309049d2db76",
   "metadata": {},
   "source": [
    "The names of restaurants are promised to be distinct; however, for foreign key relationships, you may wish to normalize using a short index number standing for the restaurants uniformly.  The separate ratings should definitely be stored as distinct data items in a relevant table.  To get a feel for more fleshed out data, invent timestamps for the reviews, such that each is mostly distinct.  A real-world data set will generally contain review dates; for the example no specific dates are required, just the form of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90d87d-7d79-419b-9af9-97f031145853",
   "metadata": {},
   "source": [
    "Although this data is small enough that performance will not be a concern, think about what indices are likely to be useful in a hypothetical version of this data that is thousands or millions of times larger.  Imagine you are running a popular restaurant review service and you want your users to have fast access to their common queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ece45f-1418-487e-8750-2c1b6438d251",
   "metadata": {},
   "source": [
    "Using the relational version of your data model, answer some simple queries, most likely using SQL.\n",
    "\n",
    "* What restaurant received the most reviews?\n",
    "* What restaurants received reviews of 10 during a given time period (the relevant range will depend on which dates you chose to populate)?\n",
    "* What style of cuisine received the highest mean review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be7d45-b7c7-48cc-91e4-491ee172d873",
   "metadata": {},
   "source": [
    "For extra credit, you may go back and write code to answer the same questions using only the key/value data model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d4270-336f-4fc4-b7c2-862e5fcc233f",
   "metadata": {},
   "source": [
    "## Other Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee9c18-c272-4688-8dc0-941c2b8ea14b",
   "metadata": {},
   "source": [
    "We present here two exercises.  One of them deals with a custom binary format, the other with web scraping.  Not every topic of this chapter is addressed in the exercises, but these two are important domains for practical data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb39c2ee-ca79-438e-9422-741ea0a95df9",
   "metadata": {},
   "source": [
    "### Enhancing the NPY Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320db7d1-91b5-4708-8e0b-14bfde27095c",
   "metadata": {},
   "source": [
    "The binary data we read from the NPY was in the simplest format we could choose.  For this exercise you want to process a somewhat more complex binary file using your own code.  Write a custom function that reads a file into a NumPy array, and test it against several arrays you have serialized using `numpy.save()` or `numpy.savez()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822889dc-3601-4030-bf9d-3b81dbb5e2c0",
   "metadata": {},
   "source": [
    "Test cases for your function are at the URLs:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/students.npy\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/students.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168de17-045b-4dff-bfa0-18b7f19e76e9",
   "metadata": {},
   "source": [
    "We have not previously looked at the NPZ format, but it is a zip archive of one or more NPY files, allowing both compression and storage of multiple arrays.  Ideally your function will handle both formats, and will determine which type of file you are reading based on the magic string in the first few bytes.  As a first pass, only try to parse the NPY version, then enhance from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a829fb9-91c6-4bf6-a348-3b18ff0fd852",
   "metadata": {},
   "source": [
    "Using the official readers, we can see that this array adds something the earlier example had not.  Specifically, it stores a `recarray` that combines several data types into each value in the array.  The rules we described earlier in this chapter will actually still suffice, but you have to think about them carefully.  The data we want to match in your reader will be exactly the same as using the official reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300948d3-1d94-4e0b-abdd-5773a4200689",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = np.load(open('data/students.npy', 'rb'))\n",
    "print(students)\n",
    "print(\"\\nDtype:\", students.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178432e9-ac33-4164-ae02-1f566224ea74",
   "metadata": {},
   "source": [
    "When you move on to processing the NPZ format, you can compare again with the official reader.  As mentioned, this might have several arrays inside it, although only one is stored in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093587a3-73b5-49cf-9ac5-9cf0af381d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = np.load(open('data/students.npz', 'rb'))\n",
    "print(arrs)\n",
    "arrs.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042ab35-4d44-4955-81af-b447350b2c3a",
   "metadata": {},
   "source": [
    "The contents of `arr_0` within the NPZ file is identical to the single array in the NPY.  However, after you have successfully parsed this NPZ file, try creating one or more others that actually do store multiple arrays, and parse those using custom code.  Decide on the best API to use for a function that may need to return either one or several arrays.  For this part of the task, the Python standard library module `zipfile` will be very helpful for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865aa327-c171-43f3-861f-0768173f1136",
   "metadata": {},
   "source": [
    "There is no reason this exercise has to be performed in Python.  Other programming languages are perfectly well able to read binary data, and the general steps involved will be very similar to those performed in this chapter in the Binary Serialized Data Structures section.  You could, for example, read the data within an NPY file into an R array instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269b274-692c-4bd5-8ea8-3f651411efd7",
   "metadata": {},
   "source": [
    "### Scraping Web Traffic\n",
    "\n",
    "The author's web domain, gnosis.cx, has been operating for more than two decades, and retains most of the \"Web 0.5\" technology and visual style it was first authored with.  One thing the web host provides, as do most others, is reports on traffic at the site (using nearly as ancient styling as that of the domain itself).  You can find the most current reports at:\n",
    "\n",
    "> https://www.gnosis.cx/stats/\n",
    "\n",
    "A snapshot of the reports current at the time of this writing are also copied to:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2911c2-4eba-4f5e-a1eb-b29b2702e652",
   "metadata": {},
   "source": [
    "An image of the report page at the time of writing is below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae837022-7b9e-44bc-8500-aa69cd4ab3c5",
   "metadata": {},
   "source": [
    "<img src=\"img/gnosis-traffic.png\" alt=\"Traffic report for gnosis.cx\" width=\"50%\"/>\n",
    "\n",
    "__Image: Traffic Report for gnosis.cx__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d014d-2309-4547-9fd8-65f385f4acbe",
   "metadata": {},
   "source": [
    "The weekly table shown is quite long since it goes back to February 2010.  The actual site is a decade older than that, but servers and logging databases were modified, losing older data.  There is also a rather large glitch of almost five years in the middle where traffic shows as zero.  The rather dramatic fall in traffic over the six weeks up to the snapshot reflects a change to using a CDN proxy for DNS and SSL (hence hiding traffic from the actual web host)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b8152-3a86-4508-aa6b-bc50d22e0a23",
   "metadata": {},
   "source": [
    "Your goal in this exercise is to write a tool to dynamically scrape the data made available in the various tables listing traffic sliced by different time increments and recurring periods (which day of week, which month of year, etc).  As part of this exercise, have your scripts generate less terrible graphs than the one shown in the screen picture (meaningless false perspective in a line graph offends good sensibility, and the apparent negative spike to negative traffic around the start of 2013 is merely inexplicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d951eb-7a24-4785-9ddc-6591ff81df64",
   "metadata": {},
   "source": [
    "It is a common need to scrape a website similar to these reports.  The pattern of having a regular and infrequently changed structure but updated contents on a daily basis, often reflects a data acquisition requirement.  A script like you will write in this exercise could run on a cronjob or under a similar mechanism, to maintain local copies and revisions of such rolling reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c705750-8f3b-47e3-91ed-74ac6d288f15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Anomoly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9ee73-f180-4356-bbb1-0dfc05ff0219",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The two exercises in this chapter ask you to look for anomalies first in quantitative data, then in categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad5745-b95a-4450-9d2f-a3207eab129e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### A Famous Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f5253-9ff2-4980-a19f-ff84ac8f4559",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The Michelson–Morley experiment was an attempt in the late 19th century to detect the existence of the *luminiferous aether*, a widely assumed medium that would carry light waves.  This was the most famous \"failed experiment\" in the history of physics in that it did not detect what it was looking for—something we now know not to exist at all.  The general idea was to measure the speed of light under different orientations of the equipment relative to the direction of movement of the earth, since relative movement of the ether medium would add or subtract from the speed of the wave.  Yes, it does not work that way under the theory of relativity, but it was a reasonable guess 150 years ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b7884-3b5c-49e6-9f3f-b9d7ab1eed45",
   "metadata": {},
   "source": [
    "Apart from the physics questions, the data set derived by the Michelson-Morley experiment is widely available, including as a sample built into R.  The same data is available at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/morley.dat\n",
    "\n",
    "Figuring out the format, which is not complex, is a good first step of this exercise (and typical of real data science work)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ed5e8-9aae-47e1-b3df-b40c4225e1bd",
   "metadata": {},
   "source": [
    "The specific numbers in this data are measurements of the speed of light in km/s with a zero point of 299,000.  So, for example, the mean measurement in experiment 1 was 299,909 km/s.  Let us look at the data in the R bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aab0005b-c665-4a88-8721-eefafb2d1116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`summarise()` ungrouping output (override with `.groups` argument)\n",
      "\u001b[90m# A tibble: 5 x 3\u001b[39m\n",
      "   Expt  Mean Count\n",
      "  \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m     1  909     20\n",
      "\u001b[90m2\u001b[39m     2  856     20\n",
      "\u001b[90m3\u001b[39m     3  845     20\n",
      "\u001b[90m4\u001b[39m     4  820.    20\n",
      "\u001b[90m5\u001b[39m     5  832.    20\n"
     ]
    }
   ],
   "source": [
    "%%R -o morley\n",
    "data(morley)\n",
    "morley %>%\n",
    "    group_by(`Expt`) %>%\n",
    "    summarize(Mean = mean(Speed), Count = max(Run))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3b0af-75a5-4676-a91c-66e2acad02c0",
   "metadata": {},
   "source": [
    "In the summary, we just look at the number of runs of each experimental setup, and the mean across that setup.  The raw data has 20 measurements within each setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9f67f-cfb3-48bb-969f-a42601d0634e",
   "metadata": {},
   "source": [
    "Using whatever programming language and tools you prefer, identify the outliers first within each setup (defined by an `Expt` number) and then within the data collection as a whole.  The hope in the original experiment was that each setup would show a significant difference in central tendency, and indeed their means are somewhat different.  This book and chapter does not explore confidence levels and null hypotheses in any detail, but create a visualization that aids you in gaining visual insight into how much apparent difference exists between the several setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f34fc-f472-4c79-b43f-2f5e568cf5cd",
   "metadata": {},
   "source": [
    "If you discard the outliers within each setup, are the differences between setups increased or decreased? Answer with either a visualization or by looking at statistics on the reduced groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720516c-a29a-4d0c-a6aa-ffb7d6cc5e8e",
   "metadata": {},
   "source": [
    "### Misspelled Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773262f-937f-400d-be3e-f0dd36b27004",
   "metadata": {},
   "source": [
    "For this exercise we return to the 25,000 human measurements we have used to illustrate a number of concepts.  However, in this variation of the data set, each row has a person's first name (pulled from the US Social Security Agency list of common first names over the last century; apologies that the names lean Anglocentric because of the past history of US population and immigration trends)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e25f54-1467-46f0-859a-db66239b2f58",
   "metadata": {},
   "source": [
    "The data set for this exercise can be found at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/humans-names.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb40f3-6567-4ddf-9579-0d788dec1b52",
   "metadata": {},
   "source": [
    "Unfortunately, our hypothetical data collectors for this data set are simply terrible typists, and they make typos when entering names with alarming frequency.  There are some number of intended names in this data set, but quite a few simple miscodings of those names as well.  The problem is: how do we tell a real name from a typo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962f9db-c843-4a34-932f-8c866c6cf92d",
   "metadata": {},
   "source": [
    "There are a number of ways to measure the similarity of strings, and that provide a clue as to likely typos.  One general class of approach is in terms of *edit distance* between strings. The R package **stringdist**, for example provides Damerau-Levenshtein, Hamming, Levenshtein, and optimal sting alignment, as measures of edit distance.  Less edit-specific fuzzy matching techniques utilize a \"bag of n-grams\" approach, and include q-gram, cosine distance, and Jaccard distance. Some heuristic metrics like Jaro and Jaro-Winkler are also included in `stringdist` along with the other measures mentioned.  Soundex, soundex variants, and metaphone look for similarity of the sounds of words as pronounced, but are therefore specific to language and even regional dialect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae006ea-345b-48aa-9260-9f09494b7f82",
   "metadata": {},
   "source": [
    "In a reversal of the more common pattern of Python versus R libraries, Python is the one that scatters string similarity measures over numerous libraries, each including just a few measures.  However, **python-Levenshtein** is a very nice package including most of these measures.  If you want cosine similarity, you may have to use `sklearn.metrics.pairwise` or another module.  For phonetic comparisons, **fonetika** and **soundex** both support multiple languages (but different languages for each; English is in common for almost all packages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48120b10-6017-4c26-a3b9-8c9024fc626c",
   "metadata": {},
   "source": [
    "On my personal system, I have a command-line utility called `similarity` that I use to measure how close strings are to each other.  This particular few line script measures Levenshtein distance, but also normalizes it to the length of the longer string.  A short name will have a small numeric measure of distance, even betweeen dissimilar strings, while long strings that are close overall can have a larger measure before normalization (depending on what measure is chosen, but for most of them).  A few examples show this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d919f49a-4c9b-4c7b-a4fa-56b025d1a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 1\n",
      "Similarity ratio: 0.8\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "similarity David Davin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c547558-8d01-4aaf-b5a9-291444f5e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 3\n",
      "Similarity ratio: 0.4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "similarity David Maven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4eb71bc6-d887-44eb-9e03-f0a6525e97fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 5\n",
      "Similarity ratio: 0.814814814815\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "similarity \"the quick brown fox jumped\" \\\n",
    "           \"thee quikc brown fax jumbed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c43e0-3c26-4207-9dd1-010ff2f9bb73",
   "metadata": {},
   "source": [
    "For this exercise, your goal is to identify every *genuine* name, and correct all the misspelled ones to the correct canonical spelling.  Keep in mind that sometimes multiple legitimate names are actually close to each other in terms of similarity measures.  However, it is probably reasonable to assume that *rare* spellings are typos, at least if they are also relatively similar to common spellings.  You may use whatever programming language, library, and metric you feel is the most useful for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf38da5-fbb2-4f4f-91ff-0eaf2fe401b2",
   "metadata": {},
   "source": [
    "Reading in the data, we see it is similar to the human measures we have seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02e08706-afc8-4765-bbde-a0efd9c72390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td>167.089607</td>\n",
       "      <td>64.806216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David</td>\n",
       "      <td>181.648633</td>\n",
       "      <td>78.281527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbara</td>\n",
       "      <td>176.272800</td>\n",
       "      <td>87.767722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John</td>\n",
       "      <td>173.270164</td>\n",
       "      <td>81.635672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michael</td>\n",
       "      <td>172.181037</td>\n",
       "      <td>82.760794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name      Height     Weight\n",
       "0    James  167.089607  64.806216\n",
       "1    David  181.648633  78.281527\n",
       "2  Barbara  176.272800  87.767722\n",
       "3     John  173.270164  81.635672\n",
       "4  Michael  172.181037  82.760794"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.read_csv('data/humans-names.csv')\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f6cb6-2003-43d7-bc40-5339238db81e",
   "metadata": {},
   "source": [
    "It is easy to see that some \"names\" occur very frequently and others only rarely.  Look at the middling values as well in working on this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25363352-8691-4991-b73e-3a6ec9cdec13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elizabeth    1581\n",
       "Barbara      1568\n",
       "Jessica      1547\n",
       "Jennifer     1534\n",
       "             ... \n",
       "Josep           1\n",
       "iWlliam         1\n",
       "Joseeph         1\n",
       "eJennifer       1\n",
       "Name: Name, Length: 249, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.Name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccf8a0-5fe4-49d3-b5c9-ea6ba3f682ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935574a-7494-4794-9619-dae00d6e08b3",
   "metadata": {},
   "source": [
    "For the exercises of this chapter, we first ask you to perform a typical multi-step data cleanup using techniques you have learned.  For the second exercise asks you to try to characterize sample bias in a provided data set using analytic tools this book has addressed (or others of your choosing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b41498-5c1b-4420-931b-3088b1e75476",
   "metadata": {},
   "source": [
    "### Data Characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efde197-4e21-4a59-b66d-b6bf3b48ff9d",
   "metadata": {},
   "source": [
    "For this exercise, you will need to perform a fairly complete set of data cleaning steps.  The focus is on techniques discussed in this chapter, but concepts discussed in other chapters will be needed as well.  Some of these tasks will require skills discussed in later chapters, so skip ahead briefly, as needed, to complete the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84eaf9-a235-4ea6-99f5-c3d8c46ffb57",
   "metadata": {},
   "source": [
    "Here we return to the \"Brad's House\" temperature data, but in its raw form.  The raw data consists of four files, corresponding to the four thermometers that were present.  These files may be found at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/outside.gz<br/>\n",
    "> https://www.gnosis.cx/cleaning/basement.gz<br/>\n",
    "> https://www.gnosis.cx/cleaning/livingroom.gz<br/>\n",
    "> https://www.gnosis.cx/cleaning/lab.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd0ea3-04e2-405e-81ed-e5f437e458de",
   "metadata": {},
   "source": [
    "The format of these data files is a simple but custom textual format.  You may want to refer back to chapter 1 (*Data Ingestion – Tablar Formats*) and to chapter 3 (*Data Ingestion – Repurposing Data Sources*) for inspiration on parsing the format.  Let us look at a few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c8b2884d-aa62-4f01-a636-7133e23bbf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003 07 26 19 28 25.200000\n",
      "2003 07 26 19 31 25.200000\n",
      "2003 07 26 19 34 25.300000\n",
      "2003 07 26 19 37 25.300000\n",
      "2003 07 26 19 40 25.400000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "zcat data/glarp/lab.gz | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22e4ae-1a74-4326-b2cf-eec6ce6f2bf6",
   "metadata": {},
   "source": [
    "As you can see, the space separated fields represent the components of a datetime, followed by a temperature reading.  The format itself is consistent for all the files.  However, the specific timestamps recorded in each file is not consistent.  All four data files end on 2004-07-16T15:28:00, and three of them begin on 2003-07-25T16:04:00. Various and different timestamps are missing in each file.  For comparison, we can recall that the full data frame we read with a utility function that performs some cleanup has 171,346 rows.  In contrast, the line counts of the several data files are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "443985a5-4a3b-47d7-bfa8-ca11e79e5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/glarp/basement.gz: 169516\n",
      "data/glarp/lab.gz: 168965\n",
      "data/glarp/livingroom.gz: 169516\n",
      "data/glarp/outside.gz: 169513\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for f in data/glarp/*.gz; do \n",
    "    echo -n \"$f: \"\n",
    "    zcat $f | wc -l \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a1a54-e2fe-42e2-aaab-7b226cb91642",
   "metadata": {},
   "source": [
    "All of the tasks in this exercise are agnostic to the particular programming languages and libraries you decide to use.  The overall goal will be to characterize each of the 685k data point as one of several conceptual categories that we present below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1915b22-7fc2-4c0b-a3b7-d69f2dbd01ed",
   "metadata": {},
   "source": [
    "**Task 1**: Read all four data files into a common data frame.  Moreover, we would like each record to be identified by a proper native timestamp rather than by separated components.  You may wish to refer forward to chapter 7 (*Feature Engineering*) which discusses date/time fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ecf926-ec76-4843-bd68-3c4c0abacd77",
   "metadata": {},
   "source": [
    "**Task 2**: Fill in all missing data points with markers indicating they are explicitly missing.  This will have two slightly different aspects.  There are some implied timestamps that do not exist in any of the data files.  Our goal is to have 3 minute increments over the entire duration of the data.  In the second aspect, some timestamps are represented in some data files but not in others.  You may wish to refer to the \"Missing Data\" section of this chapter and the same-named one in chapter 4 (*Anomaly Detection*); as well, chapter 7 discussion of date/time fields is likely relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3b307-44e0-4e6f-9843-cf2958b7d6c6",
   "metadata": {},
   "source": [
    "**Task 3**: Remove all regular trends and cycles from the data.  The relevant techniques may vary between the different instruments.  As we noted in the discussion in this chapter, three measurement series are of indoor temperatures regulated, at least in part, by thermostat, and one is of outdoors temperatures.  Whether or not the house in question had differences in thermostats or heating systems between rooms is left for readers to try to determine based on the data (at very least though, heat circulation in any house is always imperfect and not uniform).\n",
    "\n",
    "Note: As a step in performing detrending, it may be useful to temporarily impute missing data, as is discussed in chapter 6 (*Value Imputation*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0b0ae-d320-44ef-a1d4-485541d6a19b",
   "metadata": {},
   "source": [
    "**Task 4**: Characterize every data point (timestamp and location) according to these categories:\n",
    "\n",
    "* \"Regular\" data point that falls within generally expected bounds.\n",
    "* \"Interesting\" data point that is likely to indicate relevant deviation from trends.\n",
    "* \"Data error\" that reflects an improbable value relative to expectations, and is more likely to be a recording or transcription error.  Consider that a given value may be improbable based on its delta from nearby values and not exclusively because of absolute magnitude.  Chapter 4 is likely to be relevant here.\n",
    "* Missing data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468c18c-be34-45b4-ba32-1c5ad0733cef",
   "metadata": {},
   "source": [
    "**Task 5**: Describe any patterns you find in the distribution of characterized data points.  Are there temporal trends or intervals that show most or all data characterized in a certain way? Does this vary by which of four instruments we look at?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5920b3-8a82-477e-9d02-a10dc3e3886d",
   "metadata": {},
   "source": [
    "### Oversampled Polls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c42ea-894d-499c-b922-31b77e094405",
   "metadata": {},
   "source": [
    "Polling companies often deliberately utilize oversampling (overselection) in their data collection.  This is a somewhat different issue than the overweighting discussed in a topic of this chapter, or than the mechanical oversampling addressed in chapter 6 (*Value Imputation*).  Rather, the idea here is that a particular class, or a value range, is known to be uncommon in the underlying population, and hence the overall parameter space is likely to be sparsely filled for that segment of the population.  Alternately, the oversampled class may be common in the population but also represents a subpopulation about which the analytic purpose needs particularly high discernment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea789b56-b5ad-451c-9b1d-6470ae16893d",
   "metadata": {},
   "source": [
    "Use of oversampling in data collection itself is not limited to human subjects surveyed by polling companies.  There are times when it similarly makes sense for entirely unrelated subject domains; e.g. the uncommon particles produced in cyclotrons or the uncommon plants in a studied forest.  Responsible data collectors, such as the Pew Research Center that collected the data used in this exercise, will always explicitly document their oversampling methodology and expectations about the distribution of the underlying population.  You can, in fact, read all of those details about the 2010 opinion survey we utilize at:\n",
    "\n",
    "> https://www.pewsocialtrends.org/2010/02/24/millennials-confident-connected-open-to-change/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1b332-fc0b-44f1-b2d6-b365b714bd34",
   "metadata": {},
   "source": [
    "However, to complete this exercise, we prefer you skip initially consulting that documentation.  For the work here, pretend that you received this data without adequate accompanying documentation and metadata (just to be clear: Pew is meticulous here).  Such is all too often the case in the real world of messy data.  The raw data, with no systematic alteration to introduce bias or oversampling, is available by itself at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/pew-survey.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dda11e-c3bb-4280-92ba-6ee3cef40e33",
   "metadata": {},
   "source": [
    "**Task 1**: Read in the data, and make a judgement about what ages were deliberately over- or undersampled, and to what degree.  We may utilize this weighting in later synthetic sampling or weighting, but for now simply add a new column called `sampling_multiplier` to each observation of the data set matching your belief.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6305e2-ed09-4c89-93b2-701ffc460a90",
   "metadata": {},
   "source": [
    "For this purpose, treat 1x as the \"neutral\" term.  So, for example, if you believe 40 year old subjects were overselected by 5x, assign the multiplier 5.0.  Symmetrically, if you believe 50 year olds were systematically underselected by 2x, assign the multiplier 0.5.  Keep in mind that humans in the United States in 2010 were not uniformly distributed by age.  Moreover, with a sample size of about 2000 and 75 different possible ages, we expect some non-uniformity of subgroup sizes simply from randomness.  Merely random variation from the neutral selection rate should still be coded as 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e96811-f236-44c8-90a3-68c0b679c777",
   "metadata": {},
   "source": [
    "**Task 2**: Some of the categorical fields seem to encode related but distinct binary values.  For example, this question about technology is probably not ideally coded for data science goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e67576c-b578-48ce-a542-e7000f61e0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New technology makes people closer to their friends and family',\n",
       " 'New technology makes people more isolated',\n",
       " '(VOL) Both equally',\n",
       " \"(VOL) Don't know/Refused\",\n",
       " '(VOL) Neither equally']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pew = pd.read_csv('data/pew-survey.csv')\n",
    "list(pew.q23a.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060eaf56-87ea-4f66-b668-835d73fc157e",
   "metadata": {},
   "source": [
    "Since the first two descriptions may either be mutually believed or neither believed by a given surveyed person, encoding each as a separate boolean value makes sense.  How to handle a refusal to answer is an additional decision for you to make in this reencoding.  Determine which categorical values should better be encoded as multiple booleans, and modify the data set accordingly.  Explain and justify your decisions about each field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eded212-0052-4c48-addc-0f44f75348a0",
   "metadata": {},
   "source": [
    "**Task 3**: Determine whether any other demographic fields than age were oversampled.  While the names of columns are largely cryptic, you can probably safely assume that a field with qualitative answers indicating degree of an opinion are dependent variables surveyed rather than demographic independent variables.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0fd8a86-00c8-4d52-a4ba-0b4db825b447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Very happy', 'Pretty happy', 'Not too happy', \"(VOL) Don't know/Refused\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pew.q1.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd58e8f-362d-46a9-a880-9d4cf37b8f9f",
   "metadata": {},
   "source": [
    "You may need to consult outside data sources to make judgements for this task.  For example, you should be able to find the rough population distribution of U.S. timezones (in 2010) to compare to the data set distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01f941a5-b576-40b1-873a-19b8e3e72a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eastern', 'Central', 'Mountain', 'Pacific']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pew.timezone.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddcdb9c-005a-4f60-807d-e26d6e6fecac",
   "metadata": {},
   "source": [
    "**Task 4**: Some fields, such as `q1` presented in Task 3 are clearly ordinally encoded.  While it is not directly possible to assign relative ratios for (Very happy:Pretty happy) versus (Pretty happy:Not too happy), the ranking of those three values is evident, and calling them ordinal 1, 2, and 3 is reasonable and helpful.  You will, of course, also have to encode refusal to answer in some fashion.  Re-encode all relevant fields to take advantage of this intuitive domain knowledge you have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
