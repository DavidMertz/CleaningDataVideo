{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945f9adf-0027-4add-825a-21b520b80d35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a3ec7-2bb6-4df1-9b14-0a8fb818d77e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Tidy Data from Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb51395-1700-4967-ba54-a9235bbac053",
   "metadata": {},
   "source": [
    "An Excel spreadsheet with some brief information on awards given to movies is available at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/Film_Awards.xlsx\n",
    "\n",
    "In a more fleshed out case, we might have data for many more years, more types of awards, more associations that grant awards, and so on.  While the organization of this spreadsheet is much like a great many you will encounter \"in the wild,\" it is very little like the tidy data we would rather work with.  In the simple example, only 63 data values occur, and you could probably enter them into the desired structure by hand as quickly as coding the transformations.  However, the point of this exercise is to write programming code that could generalize to larger data sets of similar structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a7b14-1755-4996-815a-ccc52f5ca3fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"img/Film_Awards.png\" alt=\"Film Awards\"/>\n",
    "\n",
    "__Image: Film Awards Spreadsheet__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a9a8a-37a6-40c8-a448-c61d6eb5bc5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Your task in this exercise is to read this data into a single well-normalized data frame, using whichever language and library you are most comfortable with.  Along the way, you will need to remediate whatever data integrity problems you detect.  As examples of issues to look out for:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21690088-d166-44cf-b635-b7874a48f1b3",
   "metadata": {},
   "source": [
    "* The film _1917_ was stored as a number not a string when naÃ¯vely entered into a cell.\n",
    "* The spelling of some values is inconsistent.  Olivia Colman's name is incorrectly transcribed as 'Coleman' in one occurrence.  There is a spacing issue in one value you will need to identify.\n",
    "* Structurally, an apparent parallel is not really so.  Person names are sometimes listed under the name of the association, but elsewhere under another column.  Film names are sometimes listed under association, other times elsewhere.\n",
    "* Some column names occur multiple times in the same tabular area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6bcf7-a605-45f7-9c24-d85f6a84c593",
   "metadata": {},
   "source": [
    "In thinking about a good data frame organization, think of what the independent and dependent variables are.  In each year, each association awards for each category. These are independent dimensions.  A person name and a film name are slightly tricky since they are not exactly independent, but at the same time some awards are to a film and others to a person.  Moreover, one actor might appear in multiple films in a year (not in this sample data; but do not rule it out).  Likewise, at times multiple films have used the same name at times in film history. Some persons are both director and actor (in either the same or different films)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e28898-2820-4688-b565-ceec459672e0",
   "metadata": {},
   "source": [
    "Once you have a useful data frame, use it to answer these questions in summary reports:\n",
    "\n",
    "* For each film involved in multiple awards, list the award and year it is associated with.\n",
    "* For each actor/actress winning multiple awards, list the film and award they are associated with.\n",
    "* While not occurring in this small data set, sometimes actors/actresses win awards for multiple films (usually in different years).  Make sure your code will handle that situation.\n",
    "* It is manual work, but you may want to research and add awards given in other years; in particular, adding some data will show actors with awards for multiple films.  Do your other reports correctly summarize the larger data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360c680-2a0f-42b9-a873-7b9ec7156648",
   "metadata": {},
   "source": [
    "### Tidy Data from SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db58cd-080f-4bee-949a-54f87484c2c7",
   "metadata": {},
   "source": [
    "An SQLite database with roughly the same brief information as in the prior spreadsheet is available at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/Film_Awards.sqlite\n",
    "\n",
    "However, the information in the database version is relatively well normalized and typed.  Also, additional information has been included on a variety of entities included in the spreadsheet.  Only slightly more information is included in this schema than in the spreadsheet, but it should be able to accommodate a large amount of data on films, actors, directors, and awards, and the relationships among those data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d014f-0a24-4914-8e5d-5e30c5c20e41",
   "metadata": {},
   "source": [
    "```sql\n",
    "sqlite> .tables\n",
    "actor     award     director  org_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c497f33-235f-46dc-b217-66d4a1082aa8",
   "metadata": {},
   "source": [
    "As was mentioned in the prior exercise, the same name for a film can be used more than once, even by the same director.  For example  Abel Gance, used the title _J'accuse!_ for both his 1919 and 1938 films with connected subject matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36568c3-f289-42e0-aa12-96b81f8fa4b9",
   "metadata": {},
   "source": [
    "```\n",
    "sqlite> SELECT * FROM director WHERE year < 1950;\n",
    "Abel Gance|J'accuse!|1919\n",
    "Abel Gance|J'accuse!|1938\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84835ef-3cfe-48a0-9231-42c4df80db8e",
   "metadata": {},
   "source": [
    "Let us look at a selection from the `actor` table, for example.  In this table we have a column `gender` to differentiate beyond name. As of this writing, no transgender actor has been nominated for a major award both before and after a change in gender identity, but this schema allows for that possibility.  In any case, we can use this field to differentiate the \"actor\" versus \"actress\" awards that many organizations grant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699497c-20a4-4534-9f8d-4cc8b95c5e63",
   "metadata": {},
   "source": [
    "```sql\n",
    "sqlite> .schema actor\n",
    "CREATE TABLE actor (name TEXT, film TEXT, year INTEGER, gender CHAR(1));\n",
    "\n",
    "sqlite> SELECT * FROM actor WHERE name=\"Joaquin Phoenix\";\n",
    "Joaquin Phoenix|Joker|2019|M\n",
    "Joaquin Phoenix|Walk the Line|2006|M\n",
    "Joaquin Phoenix|Hotel Rwanda|2004|M\n",
    "Joaquin Phoenix|Her|2013|M\n",
    "Joaquin Phoenix|The Master|2013|M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb4d9c-ffd5-480b-938f-f3def00d15b8",
   "metadata": {},
   "source": [
    "The goal in this exercise is to create the same tidy data frame that you created in the prior exercise, and answer the same questions that were asked there.  If some questions can be answered directly with SQL, feel free to take that approach instead.  For this exercise, only consider awards for the years 2017, 2018, and 2019.  Some others are included in an incomplete way, but your reports are for those years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7040ef9-b557-457c-8a79-46f1bf8519a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```sql\n",
    "sqlite> SELECT * FROM award WHERE winner=\"Frances McDormand\";\n",
    "Oscar|Best Actress|2017|Frances McDormand\n",
    "GG|Actress/Drama|2017|Frances McDormand\n",
    "Oscar|Best Actress|1997|Frances McDormand\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233bbb9-d70a-41c9-b789-eac8a1d46cc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hierarchical Data\n",
    "\n",
    "The two exercises here deal first with refining the processing of the geographic data that is available in several formats.  The second exercise addresses moving between a key/value and relational model for data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb863c26-671e-471f-850f-cff949a719f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exploring Filled Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933eadf-9811-4076-8087-cf1d17821a88",
   "metadata": {},
   "source": [
    "Using the United States county data we created tidy data frames that contained the extents of counties as simple cardinal direction limits; we also were provided with the \"census area\" of each county.  Unfortunately, the data available here does not specifically address water bodies and their sizes, which might occur within counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec04062-5449-4b31-821b-70bc1fe45f1e",
   "metadata": {},
   "source": [
    "The census data can be found at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.json\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.kml\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/gz_2010_us_050_00_20m.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce83fad-e78e-41ea-9d4a-82b78d6e9caf",
   "metadata": {},
   "source": [
    "In this exercise you will create an additional column in the data frame illustrated in the text to hold the percentage of the \"bounding box\" of a county that is occupied by the census area.  The trick, of course, is that the surface area enclosed by latitude/longitude corners, is not a simple rectangle, nor even a trapezoid, but rather a portion of a spherical surface.  County shapes themselves are typically not rectangular, and may include discontiguous regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2d4ce-288b-4b95-99fa-ba537cbab59a",
   "metadata": {},
   "source": [
    "To complete this exercise, you may either reason mathematically about this area (the simplifying assumption that the Earth is a sphere is acceptable) or identify appropriate GIS software to do this calculation for you.  The result of your work will be a data frame like that presented in the chapter, but with a column called `\"occupied\"` that contains 3221 floating point values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b75641-fe2b-47d1-8497-a81890b8d879",
   "metadata": {},
   "source": [
    "For extra credit you can investigate or improve a few additional data integrity issues.  The Shapefile in the zip archive is the canonical data provided by the US Census Bureau.  The code we saw in this chapter to process GeoJSON and KML actually produce slightly different results for latitude/longitude locations, at the third decimal place.  Presumably, the independent developer whom I downloaded these conversions from allowed some data error to creep in somehow.  Diagnose which version, if either, matches the original `.shp` file, and try to characterize the reason for and degree of the discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfede5b-4f5d-40dd-b44a-b24e047335ad",
   "metadata": {},
   "source": [
    "For additional extra credit, fix the `kml_county_summary()` function presented in this chapter so that it correctly handles `<MultiGeometry>` county shapes rater than skipping over them.  How often did this problem occur among the 3221 United States counties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63a3fe-1659-4825-bfba-265ee5cb4683",
   "metadata": {},
   "source": [
    "### Create a Relational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146e8db-179b-4e4b-8c41-286b99c927dd",
   "metadata": {},
   "source": [
    "The key/value data in the DBM restaurant data is organized in a manner that might provide very fast access in Redis or similar systems.  But there is certainly a mismatch with the implicit data model.  Keys have structure in their hierarchy, but it is a finite and shallow hierarchy.  Values may be of several different implicit data types; in particular, ratings are stored as strings, but they really represent sequences of small integer values.  Other fields are simple strings (albeit stored as bytes in the DBM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb14795-b07e-4923-a8cb-0043829ce097",
   "metadata": {},
   "source": [
    "The `dbm` module in the shown example uses Python's fallback \"dumb DBM\" format which does not depend on external drivers like GDBM or LDBM.  For the example with hundreds of records this is quite fast; if you wished to used millions of records, other systems would scale well and are preferred.  This \"dumb\" format actually consistes of three separate files, but sharing the `keyval.db` prefix; the three are provided as a zip archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724cda1-dbdd-4a46-93e6-71dfa0b19cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbm.whichdb('data/keyval.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9dc6f-242a-45d4-a23a-1a77598fcbf1",
   "metadata": {},
   "source": [
    "The \"dbm.dumb\" format is not necessarily portable to other programming languages.  It is, however, simple enough that you could write an adapter rather easily.  To provide the identical data in a more universal format, a CSV of the identical content is also available:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/keyval.zip\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/keyval.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a08a0-44a8-46ef-9890-70cbb4891746",
   "metadata": {},
   "source": [
    "For this assignment you should transform the key/value data in this example into relational tables, using foreign keys where appropriate, and making good decisions about data types.  SQLite is an excellent choice for a database system to target; it is discussed in chapter 1 (*Data Ingesion â Tabular Formats*).  Any other RDBMS is also a good choice if you have administrative access (i.e. table creation rights).  Before transforming the data model, you will need to clean up the inconsistencies in the hierarchical keys that were discussed in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c574e-468f-4b19-a54f-309049d2db76",
   "metadata": {},
   "source": [
    "The names of restaurants are promised to be distinct; however, for foreign key relationships, you may wish to normalize using a short index number standing for the restaurants uniformly.  The separate ratings should definitely be stored as distinct data items in a relevant table.  To get a feel for more fleshed out data, invent timestamps for the reviews, such that each is mostly distinct.  A real-world data set will generally contain review dates; for the example no specific dates are required, just the form of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90d87d-7d79-419b-9af9-97f031145853",
   "metadata": {},
   "source": [
    "Although this data is small enough that performance will not be a concern, think about what indices are likely to be useful in a hypothetical version of this data that is thousands or millions of times larger.  Imagine you are running a popular restaurant review service and you want your users to have fast access to their common queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ece45f-1418-487e-8750-2c1b6438d251",
   "metadata": {},
   "source": [
    "Using the relational version of your data model, answer some simple queries, most likely using SQL.\n",
    "\n",
    "* What restaurant received the most reviews?\n",
    "* What restaurants received reviews of 10 during a given time period (the relevant range will depend on which dates you chose to populate)?\n",
    "* What style of cuisine received the highest mean review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be7d45-b7c7-48cc-91e4-491ee172d873",
   "metadata": {},
   "source": [
    "For extra credit, you may go back and write code to answer the same questions using only the key/value data model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d4270-336f-4fc4-b7c2-862e5fcc233f",
   "metadata": {},
   "source": [
    "## Other Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee9c18-c272-4688-8dc0-941c2b8ea14b",
   "metadata": {},
   "source": [
    "We present here two exercises.  One of them deals with a custom binary format, the other with web scraping.  Not every topic of this chapter is addressed in the exercises, but these two are important domains for practical data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb39c2ee-ca79-438e-9422-741ea0a95df9",
   "metadata": {},
   "source": [
    "### Enhancing the NPY Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320db7d1-91b5-4708-8e0b-14bfde27095c",
   "metadata": {},
   "source": [
    "The binary data we read from the NPY was in the simplest format we could choose.  For this exercise you want to process a somewhat more complex binary file using your own code.  Write a custom function that reads a file into a NumPy array, and test it against several arrays you have serialized using `numpy.save()` or `numpy.savez()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822889dc-3601-4030-bf9d-3b81dbb5e2c0",
   "metadata": {},
   "source": [
    "Test cases for your function are at the URLs:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/students.npy\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/students.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168de17-045b-4dff-bfa0-18b7f19e76e9",
   "metadata": {},
   "source": [
    "We have not previously looked at the NPZ format, but it is a zip archive of one or more NPY files, allowing both compression and storage of multiple arrays.  Ideally your function will handle both formats, and will determine which type of file you are reading based on the magic string in the first few bytes.  As a first pass, only try to parse the NPY version, then enhance from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a829fb9-91c6-4bf6-a348-3b18ff0fd852",
   "metadata": {},
   "source": [
    "Using the official readers, we can see that this array adds something the earlier example had not.  Specifically, it stores a `recarray` that combines several data types into each value in the array.  The rules we described earlier in this chapter will actually still suffice, but you have to think about them carefully.  The data we want to match in your reader will be exactly the same as using the official reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300948d3-1d94-4e0b-abdd-5773a4200689",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = np.load(open('data/students.npy', 'rb'))\n",
    "print(students)\n",
    "print(\"\\nDtype:\", students.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178432e9-ac33-4164-ae02-1f566224ea74",
   "metadata": {},
   "source": [
    "When you move on to processing the NPZ format, you can compare again with the official reader.  As mentioned, this might have several arrays inside it, although only one is stored in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093587a3-73b5-49cf-9ac5-9cf0af381d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = np.load(open('data/students.npz', 'rb'))\n",
    "print(arrs)\n",
    "arrs.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a042ab35-4d44-4955-81af-b447350b2c3a",
   "metadata": {},
   "source": [
    "The contents of `arr_0` within the NPZ file is identical to the single array in the NPY.  However, after you have successfully parsed this NPZ file, try creating one or more others that actually do store multiple arrays, and parse those using custom code.  Decide on the best API to use for a function that may need to return either one or several arrays.  For this part of the task, the Python standard library module `zipfile` will be very helpful for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865aa327-c171-43f3-861f-0768173f1136",
   "metadata": {},
   "source": [
    "There is no reason this exercise has to be performed in Python.  Other programming languages are perfectly well able to read binary data, and the general steps involved will be very similar to those performed in this chapter in the Binary Serialized Data Structures section.  You could, for example, read the data within an NPY file into an R array instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269b274-692c-4bd5-8ea8-3f651411efd7",
   "metadata": {},
   "source": [
    "### Scraping Web Traffic\n",
    "\n",
    "The author's web domain, gnosis.cx, has been operating for more than two decades, and retains most of the \"Web 0.5\" technology and visual style it was first authored with.  One thing the web host provides, as do most others, is reports on traffic at the site (using nearly as ancient styling as that of the domain itself).  You can find the most current reports at:\n",
    "\n",
    "> https://www.gnosis.cx/stats/\n",
    "\n",
    "A snapshot of the reports current at the time of this writing are also copied to:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2911c2-4eba-4f5e-a1eb-b29b2702e652",
   "metadata": {},
   "source": [
    "An image of the report page at the time of writing is below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae837022-7b9e-44bc-8500-aa69cd4ab3c5",
   "metadata": {},
   "source": [
    "<img src=\"img/gnosis-traffic.png\" alt=\"Traffic report for gnosis.cx\" width=\"50%\"/>\n",
    "\n",
    "__Image: Traffic Report for gnosis.cx__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d014d-2309-4547-9fd8-65f385f4acbe",
   "metadata": {},
   "source": [
    "The weekly table shown is quite long since it goes back to February 2010.  The actual site is a decade older than that, but servers and logging databases were modified, losing older data.  There is also a rather large glitch of almost five years in the middle where traffic shows as zero.  The rather dramatic fall in traffic over the six weeks up to the snapshot reflects a change to using a CDN proxy for DNS and SSL (hence hiding traffic from the actual web host)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b8152-3a86-4508-aa6b-bc50d22e0a23",
   "metadata": {},
   "source": [
    "Your goal in this exercise is to write a tool to dynamically scrape the data made available in the various tables listing traffic sliced by different time increments and recurring periods (which day of week, which month of year, etc).  As part of this exercise, have your scripts generate less terrible graphs than the one shown in the screen picture (meaningless false perspective in a line graph offends good sensibility, and the apparent negative spike to negative traffic around the start of 2013 is merely inexplicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d951eb-7a24-4785-9ddc-6591ff81df64",
   "metadata": {},
   "source": [
    "It is a common need to scrape a website similar to these reports.  The pattern of having a regular and infrequently changed structure but updated contents on a daily basis, often reflects a data acquisition requirement.  A script like you will write in this exercise could run on a cronjob or under a similar mechanism, to maintain local copies and revisions of such rolling reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c705750-8f3b-47e3-91ed-74ac6d288f15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Anomoly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9ee73-f180-4356-bbb1-0dfc05ff0219",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The two exercises in this chapter ask you to look for anomalies first in quantitative data, then in categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad5745-b95a-4450-9d2f-a3207eab129e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### A Famous Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f5253-9ff2-4980-a19f-ff84ac8f4559",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The MichelsonâMorley experiment was an attempt in the late 19th century to detect the existence of the *luminiferous aether*, a widely assumed medium that would carry light waves.  This was the most famous \"failed experiment\" in the history of physics in that it did not detect what it was looking forâsomething we now know not to exist at all.  The general idea was to measure the speed of light under different orientations of the equipment relative to the direction of movement of the earth, since relative movement of the ether medium would add or subtract from the speed of the wave.  Yes, it does not work that way under the theory of relativity, but it was a reasonable guess 150 years ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b7884-3b5c-49e6-9f3f-b9d7ab1eed45",
   "metadata": {},
   "source": [
    "Apart from the physics questions, the data set derived by the Michelson-Morley experiment is widely available, including as a sample built into R.  The same data is available at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/morley.dat\n",
    "\n",
    "Figuring out the format, which is not complex, is a good first step of this exercise (and typical of real data science work)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ed5e8-9aae-47e1-b3df-b40c4225e1bd",
   "metadata": {},
   "source": [
    "The specific numbers in this data are measurements of the speed of light in km/s with a zero point of 299,000.  So, for example, the mean measurement in experiment 1 was 299,909 km/s.  Let us look at the data in the R bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aab0005b-c665-4a88-8721-eefafb2d1116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`summarise()` ungrouping output (override with `.groups` argument)\n",
      "\u001b[90m# A tibble: 5 x 3\u001b[39m\n",
      "   Expt  Mean Count\n",
      "  \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m     1  909     20\n",
      "\u001b[90m2\u001b[39m     2  856     20\n",
      "\u001b[90m3\u001b[39m     3  845     20\n",
      "\u001b[90m4\u001b[39m     4  820.    20\n",
      "\u001b[90m5\u001b[39m     5  832.    20\n"
     ]
    }
   ],
   "source": [
    "%%R -o morley\n",
    "data(morley)\n",
    "morley %>%\n",
    "    group_by(`Expt`) %>%\n",
    "    summarize(Mean = mean(Speed), Count = max(Run))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3b0af-75a5-4676-a91c-66e2acad02c0",
   "metadata": {},
   "source": [
    "In the summary, we just look at the number of runs of each experimental setup, and the mean across that setup.  The raw data has 20 measurements within each setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9f67f-cfb3-48bb-969f-a42601d0634e",
   "metadata": {},
   "source": [
    "Using whatever programming language and tools you prefer, identify the outliers first within each setup (defined by an `Expt` number) and then within the data collection as a whole.  The hope in the original experiment was that each setup would show a significant difference in central tendency, and indeed their means are somewhat different.  This book and chapter does not explore confidence levels and null hypotheses in any detail, but create a visualization that aids you in gaining visual insight into how much apparent difference exists between the several setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f34fc-f472-4c79-b43f-2f5e568cf5cd",
   "metadata": {},
   "source": [
    "If you discard the outliers within each setup, are the differences between setups increased or decreased? Answer with either a visualization or by looking at statistics on the reduced groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720516c-a29a-4d0c-a6aa-ffb7d6cc5e8e",
   "metadata": {},
   "source": [
    "### Misspelled Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773262f-937f-400d-be3e-f0dd36b27004",
   "metadata": {},
   "source": [
    "For this exercise we return to the 25,000 human measurements we have used to illustrate a number of concepts.  However, in this variation of the data set, each row has a person's first name (pulled from the US Social Security Agency list of common first names over the last century; apologies that the names lean Anglocentric because of the past history of US population and immigration trends)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e25f54-1467-46f0-859a-db66239b2f58",
   "metadata": {},
   "source": [
    "The data set for this exercise can be found at:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/humans-names.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb40f3-6567-4ddf-9579-0d788dec1b52",
   "metadata": {},
   "source": [
    "Unfortunately, our hypothetical data collectors for this data set are simply terrible typists, and they make typos when entering names with alarming frequency.  There are some number of intended names in this data set, but quite a few simple miscodings of those names as well.  The problem is: how do we tell a real name from a typo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962f9db-c843-4a34-932f-8c866c6cf92d",
   "metadata": {},
   "source": [
    "There are a number of ways to measure the similarity of strings, and that provide a clue as to likely typos.  One general class of approach is in terms of *edit distance* between strings. The R package **stringdist**, for example provides Damerau-Levenshtein, Hamming, Levenshtein, and optimal sting alignment, as measures of edit distance.  Less edit-specific fuzzy matching techniques utilize a \"bag of n-grams\" approach, and include q-gram, cosine distance, and Jaccard distance. Some heuristic metrics like Jaro and Jaro-Winkler are also included in `stringdist` along with the other measures mentioned.  Soundex, soundex variants, and metaphone look for similarity of the sounds of words as pronounced, but are therefore specific to language and even regional dialect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae006ea-345b-48aa-9260-9f09494b7f82",
   "metadata": {},
   "source": [
    "In a reversal of the more common pattern of Python versus R libraries, Python is the one that scatters string similarity measures over numerous libraries, each including just a few measures.  However, **python-Levenshtein** is a very nice package including most of these measures.  If you want cosine similarity, you may have to use `sklearn.metrics.pairwise` or another module.  For phonetic comparisons, **fonetika** and **soundex** both support multiple languages (but different languages for each; English is in common for almost all packages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48120b10-6017-4c26-a3b9-8c9024fc626c",
   "metadata": {},
   "source": [
    "On my personal system, I have a command-line utility called `similarity` that I use to measure how close strings are to each other.  This particular few line script measures Levenshtein distance, but also normalizes it to the length of the longer string.  A short name will have a small numeric measure of distance, even betweeen dissimilar strings, while long strings that are close overall can have a larger measure before normalization (depending on what measure is chosen, but for most of them).  A few examples show this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d919f49a-4c9b-4c7b-a4fa-56b025d1a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 1\n",
      "Similarity ratio: 0.8\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "similarity David Davin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c547558-8d01-4aaf-b5a9-291444f5e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 3\n",
      "Similarity ratio: 0.4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "similarity David Maven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4eb71bc6-d887-44eb-9e03-f0a6525e97fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 5\n",
      "Similarity ratio: 0.814814814815\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "similarity \"the quick brown fox jumped\" \\\n",
    "           \"thee quikc brown fax jumbed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c43e0-3c26-4207-9dd1-010ff2f9bb73",
   "metadata": {},
   "source": [
    "For this exercise, your goal is to identify every *genuine* name, and correct all the misspelled ones to the correct canonical spelling.  Keep in mind that sometimes multiple legitimate names are actually close to each other in terms of similarity measures.  However, it is probably reasonable to assume that *rare* spellings are typos, at least if they are also relatively similar to common spellings.  You may use whatever programming language, library, and metric you feel is the most useful for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf38da5-fbb2-4f4f-91ff-0eaf2fe401b2",
   "metadata": {},
   "source": [
    "Reading in the data, we see it is similar to the human measures we have seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02e08706-afc8-4765-bbde-a0efd9c72390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td>167.089607</td>\n",
       "      <td>64.806216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David</td>\n",
       "      <td>181.648633</td>\n",
       "      <td>78.281527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbara</td>\n",
       "      <td>176.272800</td>\n",
       "      <td>87.767722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John</td>\n",
       "      <td>173.270164</td>\n",
       "      <td>81.635672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michael</td>\n",
       "      <td>172.181037</td>\n",
       "      <td>82.760794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name      Height     Weight\n",
       "0    James  167.089607  64.806216\n",
       "1    David  181.648633  78.281527\n",
       "2  Barbara  176.272800  87.767722\n",
       "3     John  173.270164  81.635672\n",
       "4  Michael  172.181037  82.760794"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.read_csv('data/humans-names.csv')\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f6cb6-2003-43d7-bc40-5339238db81e",
   "metadata": {},
   "source": [
    "It is easy to see that some \"names\" occur very frequently and others only rarely.  Look at the middling values as well in working on this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25363352-8691-4991-b73e-3a6ec9cdec13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elizabeth    1581\n",
       "Barbara      1568\n",
       "Jessica      1547\n",
       "Jennifer     1534\n",
       "             ... \n",
       "Josep           1\n",
       "iWlliam         1\n",
       "Joseeph         1\n",
       "eJennifer       1\n",
       "Name: Name, Length: 249, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.Name.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
