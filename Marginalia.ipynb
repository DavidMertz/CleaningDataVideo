{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb29644-a8c0-47fb-be94-5e0035706b18",
   "metadata": {},
   "source": [
    "# Overall Introduction\n",
    "\n",
    "My name is David Mertz, and I've been a data scientist, writer of technical books and articles, and computer science trainer, for a great many years.\n",
    "\n",
    "Although my doctorate was in political philosophy, a topic which you may see hints of in a few allusions and references in this course, over the last 25 years I've created a number of programming and data science training programs. \n",
    "\n",
    "These programs have been used by companies such as IBM, Anaconda, and INE.  I've recorded and written courses and books for Addison Wesley, Pearson, O'Reilly, Manning, and other publishers.\n",
    "\n",
    "There are a great many excellent and wonderful books and course about machine learning, or about data analysis, or about scientific computing, or other topics in or adjacent to data science.  \n",
    "\n",
    "What almost all of these works have in common is that they quickly mention the centrality of arriving at clean (enough) data as a prerequisite to the analysis or presentation they actually wish to discuss.  Sometimes a preface, appendix, or short chapter says a few words about the topic of cleaning data.\n",
    "\n",
    "Almost nowhere else can you find a full course or book about this essential prerequisite.  This notwithstanding that all such works acknowledge that the cleaning steps make up the majority of the actual work done by data scientists.\n",
    "\n",
    "I've divided this course into six lessons.\n",
    "\n",
    "In a short preface, I provide a conceptual overview of the language and techniques used in data science, and generally how to understand the _shape_ and _feel_ of data sets.\n",
    "\n",
    "The next three longer lessons look at ingestion of data from various families of formats.  That is, data might arrive in tabular formats. You might receive data with hierarchical structures. And very often, data is something you need _extract_ in some manner from objects whose primary purpose is not to serve as data per se.\n",
    "\n",
    "The particular obstacles and attitudes you will find or adopt varies not only with the overall structure of the data sets, but also very often with the specific representations used.  Relational database management systems using SQL have different pitfalls and virtues than do PDF documents, and both are distinct from JSON data.  Ultimately, however, the kinds of analyses we wish to arrive at are largely similar.\n",
    "\n",
    "Once you have put data into somewhat normalized representations, three of the main steps you will perform before final analysis make up the final three lessons.  In rough outline, you will usually perform these steps in approximately the order these lessons are arranged in.\n",
    "\n",
    "Detecting anomolies within data is almost always necessary.  Data always arrive *dirty*.  _Et in Arcadia, Inclutus Est_.  Finding those data points that do not belong is essential to preparing the data for successful uses.\n",
    "\n",
    "However, while individual data points are often problematic, so too are general patterns within the data.  Very broadly, these patterns are called _bias_. The lesson on Data Quality addresses bias and several related whole data set distortions. We look at both how to identify these concerns and how to remediate them.\n",
    "\n",
    "The final lesson covers Value Imputation.  While you may have ruled out certain data points by techniques discussed in earlier lessons, often it is necessary to attribute, i.e. _impute_, plausible values to take their place.\n",
    "\n",
    "I thank you for choosing this course, and a very much hope and believe you will find it useful to your practices in working with data.  Let's get started with the lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978448f3-a711-478a-ac6c-4de4c9877e79",
   "metadata": {},
   "source": [
    "# Introduction to the Preface\n",
    "\n",
    "Let's look briefly at some nomenclature and concepts relevant to data cleaning. We often wish to distinguish among structural and content-related data issues, as shown with an example in the Preface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf00167-7f5a-4b2b-b2ed-e5a2203a51da",
   "metadata": {},
   "source": [
    "# Introduction to Data Ingestion - Tabular Formats\n",
    "\n",
    "In most data science, the data we wish to work with has a tabular format, ideally a so-called \"tidy\" format.  Sometimes that data starts out in a physical format that is basically tabular, but nonetheless many formats have characteristic pitfalls.\n",
    "\n",
    "CSV or fixed width data is great, but it starts as plain text and hence often has data typing ambiguities.  Spreadsheets are often used to record data we would like to utilize, but they are also prone to probably the most problems of all formats once we want to perform more precise analysis.\n",
    "\n",
    "Most specialized tabular formats avoid most problems, but are used less often.  Let's dig into all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c3241-7a85-426f-8099-e16485897059",
   "metadata": {},
   "source": [
    "# Introduction to Data Ingestion - Hierarchical Formats\n",
    "\n",
    "In this lesson, we look at hierarchical formats that data may arrive to us in.  The most common of these formats are XML, JSON, and within NoSQL databases.  \n",
    "\n",
    "In each case, a first goal is usually to convert the underlying data to a more tabular representation.  But the specific formats also each have some characteristic dangers in their specifics, and we need to think of how to treat those.\n",
    "\n",
    "XML is widely used, but fairly complex.  Good tools exist for pulling out the data points we actually care about from XML documents or streams.\n",
    "\n",
    "JSON has become ubiquitous and is generally a simple and useful format.  Sometimes JSON takes the form of JSON-lines, which represents many small documents, streamed in sequence.  JSON is useful, but is loosely structured, which often merits using JSON Schema to create more controlled hierarchies.\n",
    "\n",
    "Data also often lives in so-called NoSQL databases, sometimes called \"document databases.\"  We look, in this lesson, at utilizing data from those sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f208d5-e72e-4947-8e05-7916f3afe8b8",
   "metadata": {},
   "source": [
    "# Introduction to Data Ingestion - Repurposing Data Sources\n",
    "\n",
    "Sometimes the data we want to work with is embedded inside web pages, PDF documents, or within images.  These formats are very common, but we must perform initial work to pull out that information that is of interest to us for analytic, visualization, or modeling purposes.\n",
    "\n",
    "Follow this lesson to understand useful techniques for data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9495153-51c2-4943-a130-8aff2290bb53",
   "metadata": {},
   "source": [
    "# Introduction to Anomoly Detection\n",
    "\n",
    "When we, or our collaborators or sources, collect data, it often contains anomolous data points.  Detecting and marking these problem data is an important element of the data science pipeline.\n",
    "\n",
    "Data are sometimes simply missing, but the manners in which _missingness_ is marked varies considerably among formats and sources.  This lesson looks at several ways absence is marked, including a discussion of various sentinels that are often used, and how to work with those.\n",
    "\n",
    "Although sentinels can require work to recognize and transform, a trickier question arises with data points that are detectably wrong, but have the general form of good data.\n",
    "\n",
    "These bad data points can sometimes result from direct miscoding.  At other times, we can deduce problems based on known bounds of a data field, or be extreme variance from expectations within the data. Each of these ways that data can go wrong is discussed in a sub-lesson.\n",
    "\n",
    "As well as simple outliers which can be identified by relatively easy statistical tests, sometimes outliers need multivariate analysis to locate. The final sub-lesson of this lesson addresses multivariate outliers.\n",
    "\n",
    "Dig in, I think you'll enjoy this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f520f-cdfb-43c4-831d-df18850b6052",
   "metadata": {},
   "source": [
    "# Introduction to Data Quality\n",
    "\n",
    "In this lesson we look for systematic trends in the availability of data.  Many such trends can be characterized in statistical bias, which may in turn result from sample bias.\n",
    "\n",
    "At a first level we look at identifying the presence of such trends.  Sometimes bias is detectable from data distributions, and at other times from domain knowledge that provides expectations.  We make a minor detour to look at Benford's Law as a mechanism for detecting a certain kind of bias.\n",
    "\n",
    "Beyond detection of bias, this lesson looks at remediations.  One sub-lesson addresses class imbalance in data sets, which may result from bias or may simply reflect the underlying distribution of data but nonethless warrant sampling weighting.\n",
    "\n",
    "I final sub-lesson looks at normalization and scaling, which is often helpful in preparing data for many kinds of analysis and modeling.\n",
    "\n",
    "This lesson will provide you with tools to create a well rounded data set for final steps in your data science pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5dceac-d98b-44e4-af3f-a695a438b585",
   "metadata": {},
   "source": [
    "# Introduction to Value Imputation\n",
    "\n",
    "In this final lesson, we look at value imputation.  Following steps in earlier lessons, you may have identified or marked data points as missing or unreliable.  Very often, a followup to such marking is imputing new plausible values to that missing data.\n",
    "\n",
    "The sub-lessons here look first at imputation of \"typical values\" that might be either global or based on parameter space locality.\n",
    "\n",
    "Moving to an arguably more sophisticated technique, imputation might also reflect trends that we can identify within data.  Many trends are temporal, with timestamped data points tending to resemble those from similar timestamps.  However, many trends are also non-temporal, both by spatial location and following more abstract continuities.\n",
    "\n",
    "I final sub-lesson looks at oversampling and undersampling.  In a general sense, sampling is a mechanism to impute data based on that data we already have.  Oversampling in particular can use techniques more nuanced than mere duplication of existing data rows.\n",
    "\n",
    "Follow this lesson, and try its techniques against your own data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de5bed-89bb-424d-adff-d3bddc284dfe",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "With the tools and concepts provided in this course, you should be able to bring your data from its raw and imperfect state to one with which you can create machine learning models, produce useful data visualizations, perform statistical analyses, and the many other tasks within data science.\n",
    "\n",
    "We've looked at ingestion of numerous data format you are likely to encounter. From there, we've seen how to identify anomalies, detect bias, and remediate and impute data.\n",
    "\n",
    "The material in this course is derived from my book, _Cleaning Data for Effective Data Science_.  That text is available online at https://gnosis.cx/cleaning, or for purchase in various formats.  In the book I am able to go into deeper detail on many of the topics addressed in this course.\n",
    "\n",
    "Since the examples in this course are presented as Python, I can also recommend my Addison Wesley title _Better Python Code_, which you can find in raw form at https://gnosis.cx/better, or for purchase as a very nicely produced printed or ebook via that same link.\n",
    "\n",
    "I have not, in this course, emphasized the specific tools used in illustrating data manipulation techniques.  Most examples, however, are illustrated with code snippets.  The code samples generally utilize the popular Pandas dataframe library (https://pandas.pydata.org/), but many examples also use instead use Polars (https://pola.rs/) which I think is a much improved successor for Pandas.  For folks who prefer to use the R programming language, the Tidyverse (https://www.tidyverse.org/) tools are excellent, and can perform everything shown in this course, albeit with different syntax.\n",
    "\n",
    "I very much welcome anyone to contact me by email at mertz@gnosis.cx."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
