{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data Ingestion - Repurposing Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All language is but a poor translation.<br/>–Franz Kafka "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes data lives in formats that take extra work to ingest.  For common and explicitly data-oriented formats, common libraries already have readers built into them.  Data frame libraries, for example, read a huge number of different file types.  At worst, slightly less common formats have their own more specialized libraries that provide a relatively straightforward path between the original format and the general purpose data processing library you wish to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A greater difficulty often arises because a given format is not *per se* a data format, but exists for a different purpose.  Nonetheless, often there is data somehow embedded or encoded in the format that we would like to utilize.  For example, web pages are generally designed for human readers and rendered by web browsers with \"quirks modes\" that deal with not-quite-HTML, as is often needed.  Portable Document Format (PDF) documents are similar in having intended human readers in mind, and yet also often containing tabular or other data that we would like to process as data scientists. Of course, in both cases, we would rather have the data itself in some separate, easily ingestible, format; but reality does not always live up to our hopes.  Image formats likewise are intended for presentation of pictures to humans; but we sometimes wish to characterize or analyze collections of images in some data science or machine learning manner.  There *is* a bit of a difference between Hypertext Markup Language (HTML) and PDF on one hand, and images on the other hand.  With the former, we hope to find tables or numeric lists that are incidentally embedded inside a textual document.  With the images, we are interested in the format itself as data: what is the pattern of pixel values and what does that tell us about characteristics of the image as such?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still other formats are indeed intended as data formats, but they are unusual enough that common readers for the formats will not be available.  Generally, custom text formats are manageable, especially if you have some documentation of what the rules of the format are.  Custom binary formats are usually more work, but possible to decode if the need is sufficiently pressing and other encodings do not exist.  Mostly such custom formats are legacy in some way, and a one-time conversion to more widely used formats is the best process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Before we get to the sections of this chapter, let us run our standard setup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.setup import *\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout err\n",
    "%%R \n",
    "library(imager)\n",
    "library(tidyverse)\n",
    "library(rvest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important letters which contain no errors will develop errors in the mail.<br/>–Anonymous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepts**:\n",
    "\n",
    "* HTML tables\n",
    "* Non-tabular data\n",
    "* Command-line scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great deal of interesting data lives on web pages, and often, unfortunately, we do not have access to the same data in more structured data formats.  In the best cases, the data we are interested in at least lives within HTML tables inside of a web page; however, even where tables are defined, often the content of the cells has more than only the numeric or categorical values of interest to us.  For example, a given cell might contain commentary on the data point or a footnote providing a source for the information.  At other times, of course, the data we are interested in is not in HTML tables at all, but structured in some other manner across a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will first use the R library **rvest** to extract some tabular data, then use **BeautifulSoup** in Python to work with some non-tabular data.  This shifting tool choice is not because one tool or the other is uniquely capable of doing the task we use it for, nor even is one necessarily better than the other at it.  I simply want to provide a glimpse into a couple different tools for performing a similar task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Python world, the framework **Scrapy** is also widely used—it does both more and less than BeautifulSoup. Scrapy can actually pull down web pages, and navigate dynamically amongst them while BeautifulSoup is only interested in the parsing aspect, and it assumes you have used some other tool or library (such as **Requests**) to actually obtain the HTML resource to be parsed.  For what it does, BeautifulSoup is somewhat friendlier and is remarkably well able to handle malformed HTML.  In the real world, what gets called \"HTML\" is often only loosely conformant to any actual format standards, and hence web browsers, for example, are quite sophisticated (and complicated) in providing reasonable rendering of only vaguely structured tag soups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of this writing, in 2020, the Covid-19 pandemic is ongoing, and the exact contours of the disease worldwide are changing on a daily basis.  Given this active change, the current situation is too much of a moving target to make a good example (and too politically and ethically laden).  Let us look at some data from a past disease though to illustrate web scraping.  While there are surely other sources for similar data we could locate, and some are most likely  in immediately readable formats, we will collect our data from the Wikipedia article on the 2009 flu pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial fact about web pages is that they can be and often are modified by their maintainers.  There are times when the Wayback Machine (https://archive.org/web/) can be used to find specific historical versions. Data that is available at a given point in time may not continue to be in the future at a given URL.  Or even where a web page maintains the same underlying information, it may change details of its format that would change the functionality of our scripts for processing the page.  On the other hand, many changes represent exactly the updates in data values that are of interest to us, and the dynamicness of a web page is exactly its greatest value.  These are tradeoffs to keep in mind when scraping data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia has a great many virtues, and one of them is its versioning of its pages.  While a default URL for a given topic has a friendly and straightforward spelling that can often even be guessed from the name of a topic, Wikipedia also provides a URL parameter in its query strings that identifies an exact version of the web page that should remain bitwise identical for all time.  There are a few exceptions to this permanence; for example, if an article is deleted altogether it may become inaccessible.  Likewise if a template is part of a renaming, as unfortunately occured during the writing of this book, a \"permanent\" link can break.  Let us examine the Wikipedia page we will attempt to scrape in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same string composed over two lines for layout\n",
    "# XXXX substituted for actual ID because of discussed breakage\n",
    "url2009 = (\"https://en.wikipedia.org/w/index.php?\"\n",
    "           \"title=2009_flu_pandemic&oldid=XXXX\")\n",
    "\n",
    "# A version of the original table can still be found here: \n",
    "# https://te.wikipedia.org/wiki/%E0%B0%AE%E0%B1%82%E0%B0%B8:2009_flu_pandemic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particular part of that previous page that we are interested in is an infobox about halfway down the article.  It looks like this in my browser:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Flu2009-infobox.png\" alt=\"2009 Flu Infobox\" width=\"40%\"/>\n",
    "\n",
    "__Image: Wikipedia Infobox in Article \"2009 Flu Pandemic\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing a script for web scraping inevitably involves a large amount of trial-and-error.  In concept, it might be possible to manually read the underlying HTML before processing it, and correctly identify the positions and types of the element of interest.  In practice, it is always quicker to eyeball the partially filtered or indexed elements, and refine the selection through repetition.  For example, in this first pass, I determined that the \"cases by region\" table was number 4 on the web page by enumerating through earlier numbers and visually ruling them out.  As rendered by a web browser, it is not always apparent what element is a table; it is also not necessarily the case that an elements being rendered visually above another actually occurs earlier in the underlying HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first pass also already performs a little bit of cleanup in value names.  Through experimentation, I determined that some region names contain an HTML &lt;br/&gt; which is stripped in the below code, leaving no space between words.  In order to address that, I replace the HTML break with a space, then need to reconstruct an HTML object from the string and select the table again.\n",
    "\n",
    "```R\n",
    "page <- read_html(url2009) \n",
    "table <- page %>% \n",
    "    html_nodes(\"table\") %>%\n",
    "    .[[4]] %>%\n",
    "    str_replace_all(\"<br>\", \" \") %>%\n",
    "    minimal_html() %>%\n",
    "    html_node(\"table\") %>%\n",
    "    html_table(fill = TRUE) \n",
    "head(table, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code produced the following (before the template change issue):\n",
    "\n",
    "```\n",
    "   2009 flu pandemic data 2009 flu pandemic data 2009 flu pandemic data\n",
    "1                    Area       Confirmed deaths                   <NA>\n",
    "2       Worldwide (total)                 14,286                   <NA>\n",
    "3 European Union and EFTA                  2,290                   <NA>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the first pass still has problems, all the data is basically present, and we can clean it up without needing to query the source further.  Because of the nested tables, the same header is incorrectly deduced for each column.  The more accurate headers are relegated to the first row.  Moreover, an extraneous column that contains footnotes was created (it has content in some rows below those shown by `head()`).  Because of the commas in numbers over a thousand, integers were not inferred.  Let us convert the data.frame to a tibble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "data <- as_tibble(table, \n",
    "        .name_repair = ~ c(\"Region\", \"Deaths\", \"drop\")) %>%\n",
    "    select(-drop) %>%\n",
    "    slice(2:12) %>%\n",
    "    mutate(Deaths = as.integer(gsub(\",\", \"\", Deaths)),\n",
    "           Region = as.factor(Region))\n",
    "data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this might give us a helpful table like:\n",
    "\n",
    "```\n",
    "# A tibble: 11 x 2\n",
    "   Region                                    Deaths\n",
    "   <fct>                                      <int>\n",
    " 1 Worldwide (total)                          14286\n",
    " 2 European Union and EFTA                     2290\n",
    " 3 Other European countries and Central Asia    457\n",
    " 4 Mediterranean and Middle East               1450\n",
    " 5 Africa                                       116\n",
    " 6 North America                               3642\n",
    " 7 Central America and Caribbean                237\n",
    " 8 South America                               3190\n",
    " 9 Northeast Asia and South Asia               2294\n",
    "10 Southeast Asia                               393\n",
    "11 Australia and Pacific                        217\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this is a very small example that could easily be typed in manually.  The general techniques shown might be applied to a much larger table.  More significantly, they might also be used to scrape a table on a web page that is updated frequently.  2009 is strictly historical, but other data is updated every day, or even every minute, and a few lines like the ones shown could pull down current data each time it needs to be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our processing of a non-tabular source, we will use Wikipedia as well.  Again, a topic that is of wide interest and not prone to deletion is chosen.  Likewise, a specific historical version is indicated in the URL, just in case the page changes its structure by the time you read this.  In a slightly self-referential way, we will look at the article that lists HTTP status codes in a term/definition layout.  A portion of that page renders in my browser like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/HTTP-status-codes.png\" alt=\"HTTP status codes\" width=\"50%\"/>\n",
    "\n",
    "__Image: HTTP Status Codes, Wikipedia Definition List__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous other codes are listed in the articles that are not in the screenshot.  Moreover, there are section divisions and other descriptive elements or images throughout the page.  Fortunately, Wikipedia tends to be very regular and predictable in its use of markup.  The URL we will examine is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_http = (\"https://en.wikipedia.org/w/index.php?\"\n",
    "            \"title=List_of_HTTP_status_codes&oldid=947767948\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is actually retrieve the HTML content.  The Python standard library module `urllib` is perfectly able to do this task.  However, even its [official documentation](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) recommends using the third-party package Requests for most purposes.  There is nothing you *cannot* do with `urllib`, but often the API is more difficult to use, and is unnecessarily complicated for historical/legacy reasons. For simple things, like what is shown in this book, it makes little difference; for more complicated tasks, getting in the habit of using Requests is a good idea.  Let us open a page and check the status code returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "resp = requests.get(url_http)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw HTML we retrieved is not especially easy to work with.  Even apart from the fact it is compacted to remove extra whitespace, the general structure is a \"tag soup\" with various things nested in various places, and in which basic string methods or regular expressions do not help us very much in identifying the parts we are interested in.  For example, here is a short segment from somewhere in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'rge (RFC 7231)</dt>\\n<dd>The request is larger th'\n",
      " b'an the server is willing or able to process. Pre'\n",
      " b'viously called \"Request Entity Too Large\".<sup i'\n",
      " b'd=\"cite_ref-47\" class=\"reference\"><a href=\"#cite'\n",
      " b'_note-47\">&#91;46&#93;</a></sup></dd>\\n<dt><span '\n",
      " b'class=\"anchor\" id=\"414\"></span>414 URI Too Long '\n",
      " b'(RFC 7231)</dt>\\n<dd>The <a href=\"/wiki/URI\" clas'\n",
      " b's=\"mw-redirect\" title=\"URI\">URI</a> provided was'\n",
      " b' too long for the server to process. Often the r'\n",
      " b'esult of too much data being encoded as a query-'\n",
      " b'string of a GET request, in which case it should'\n",
      " b' be converted to a POST request.<sup id=\"cite_re'\n",
      " b'f-48\" class=\"reference\">')\n"
     ]
    }
   ],
   "source": [
    "pprint(resp.content[43400:44000], width=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would like is to make the tag soup beautiful instead.  The steps in doing so are first creating a \"soup\" object from the raw HTML, then using methods of that soup to pick out the elements we care about for our data set. As with the R and rvest version—as indeed, with any library you decide to use—finding the right data in the web page will involve trial-and-error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start at our examination, we noticed that the status codes themselves are each contained within an HTML &lt;dt&gt; element.  Below we display the first and last few of the elements identified by this tag.  Everything so identified is, in fact, a status code, but I only know that from manual inspection of all of them (fortunately, eyeballing fewer than 100 items is not difficult; doing so with a million would be infeasible).  However, if we look back at the original web page itself, we will notice that two AWS custom codes at the end are not captured because the page formatting is inconsistent for those.  In this section, we will ignore those, having determined they are not general purpose anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Continue\n",
      "101 Switching Protocols\n",
      "102 Processing (WebDAV; RFC 2518)\n",
      "103 Early Hints (RFC 8297)\n",
      "200 OK\n",
      "524 A Timeout Occurred\n",
      "525 SSL Handshake Failed\n",
      "526 Invalid SSL Certificate\n",
      "527 Railgun Error\n",
      "530\n"
     ]
    }
   ],
   "source": [
    "codes = soup.find_all('dt')\n",
    "for code in codes[:5] + codes[-5:]:\n",
    "    print(code.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if each &lt;dt&gt; were matched with a corresponding &lt;dd&gt;.  If it were, we could just read all the &lt;dd&gt; definitions and zip them together with the terms.   Real-world HTML is messy.  It turns out—and I discovered this while writing, not by planning the example—that there are sometimes more than one (and potentially sometimes zero) &lt;dd&gt; elements following each &lt;dt&gt;.  Our goal then will be to collect all of the &lt;dd&gt; elements that follow a &lt;dt&gt; until other tags occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the BeautifulSoup API, the empty space between elements is a node of plain text that contains exactly the characters (including whitespace) inside that span.  It is tempting to use the API `node.find_next_siblings()` in this task.  We *could* succeed doing this, but this method will fetch too much, including all subsequent &lt;dt&gt; elements after the current one.  Instead, we can use the property `.next_sibling` to get each one, and stop when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dds_after(node):\n",
    "    dds = []\n",
    "    sib = node.next_sibling\n",
    "    while True:     # Loop until a break\n",
    "        # Last sibling within page section\n",
    "        if sib is None:\n",
    "            break\n",
    "        # Text nodes have no element name\n",
    "        elif not sib.name: \n",
    "            sib = sib.next_sibling\n",
    "            continue\n",
    "        # A definition node\n",
    "        if sib.name == 'dd':\n",
    "            dds.append(sib)\n",
    "            sib = sib.next_sibling\n",
    "        # Finished <dd> the definition nodes\n",
    "        else:\n",
    "            break\n",
    "    return dds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom function I wrote above is straightforward, but special to this purpose.  Perhaps it is extensible to similar definition lists one finds in other HTML documents.  BeautifulSoup provides numerous useful APIs, but they are building blocks for constructing custom extractors rather than foreseeing every possible structure in an HTML document. To understand it, let us look at a couple of the status codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 Bad Request\n",
      "   The server cannot or will not process th ...\n",
      "401 Unauthorized (RFC 7235)\n",
      "   Similar to 403 Forbidden, but specifical ...\n",
      "   Note: Some sites incorrectly issue HTTP  ...\n",
      "402 Payment Required\n",
      "   Reserved for future use. The original in ...\n"
     ]
    }
   ],
   "source": [
    "for code in codes[23:26]:\n",
    "    print(code.text)\n",
    "    for dd in find_dds_after(code):\n",
    "        print(\"  \", dd.text[:40], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTTP 401 response contains two separate definition blocks. Let us apply the function across all the HTTP code numbers.  What is returned is a list of definition blocks; for our purpose we will join the text of each of these with a newline.  In fact, we construct a data frame with all the information of interest to us in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for code in codes:\n",
    "    # All codes are 3 character numbers\n",
    "    number = code.text[:3]\n",
    "    # parenthetical is not part of status\n",
    "    text, note = code.text[4:], \"\"\n",
    "    if \" (\" in text:\n",
    "        text, note = text.split(\" (\")\n",
    "        note = note.rstrip(\")\")\n",
    "    # Compose description from list of strings\n",
    "    description = \"\\n\".join(t.text for t in find_dds_after(code))\n",
    "    data.append([int(number), text, note, description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Python list of lists, we can create a Pandas DataFrame for further work on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Note</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Continue</td>\n",
       "      <td></td>\n",
       "      <td>The server has received the request headers an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Switching Protocols</td>\n",
       "      <td></td>\n",
       "      <td>The requester has asked the server to switch p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Processing</td>\n",
       "      <td>WebDAV; RFC 2518</td>\n",
       "      <td>A WebDAV request may contain many sub-requests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Checkpoint</td>\n",
       "      <td></td>\n",
       "      <td>Used in the resumable requests proposal to res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Early Hints</td>\n",
       "      <td>RFC 8297</td>\n",
       "      <td>Used to return some response headers before fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>Standard response for successful HTTP requests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Created</td>\n",
       "      <td></td>\n",
       "      <td>The request has been fulfilled, resulting in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Accepted</td>\n",
       "      <td></td>\n",
       "      <td>The request has been accepted for processing, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Text              Note  \\\n",
       "Code                                          \n",
       "100              Continue                     \n",
       "101   Switching Protocols                     \n",
       "102            Processing  WebDAV; RFC 2518   \n",
       "103            Checkpoint                     \n",
       "103           Early Hints          RFC 8297   \n",
       "200                    OK                     \n",
       "201               Created                     \n",
       "202              Accepted                     \n",
       "\n",
       "                                            Description  \n",
       "Code                                                     \n",
       "100   The server has received the request headers an...  \n",
       "101   The requester has asked the server to switch p...  \n",
       "102   A WebDAV request may contain many sub-requests...  \n",
       "103   Used in the resumable requests proposal to res...  \n",
       "103   Used to return some response headers before fi...  \n",
       "200   Standard response for successful HTTP requests...  \n",
       "201   The request has been fulfilled, resulting in t...  \n",
       "202   The request has been accepted for processing, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(data, \n",
    "              columns=[\"Code\", \"Text\", \"Note\", \"Description\"])\n",
    "    .set_index('Code')\n",
    "    .sort_index()\n",
    "    .head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the two examples this book walked through in some details are not general to all the web pages you may wish to scrape data from.  Organization into tables and into definition lists are certainly two common uses of HTML to represent data, but many other conventions might be used.  Particular domain specific—or likely page specific—`class` and `id` attributes on elements is also a common way to mark the structural role of different data elements.  Libraries like rvest, BeautifulSoup, and scrapy all make identification and extraction of HTML by element attributes straightforward as well.  Simply be prepared to try many variations on your web scraping code before you get it right.  Generally, your iteration will be a narrowing process; each stage *needs to*include the information desired, it becomes a process of removing the parts you do not want through refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command-Line Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach that I have often used for web scraping is to use the command-line web browsers `lynx` and `links`. Install either or both with your system package manager. These tools can dump HTML contents as text which is, in turn, relatively easy to parse if the format is simple.  There are many times when just looking for patterns of intentation, vertical space, searching for particular keywords, or similar text processing, will get the data you need more quickly than the trial-and-error of parsing libraries like rvest or BeautifulSoup.  Of course, there is always a certain amount of eyeballing and retrying commands.  For people who are well versed in text processing tools, this approach is worth considering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two similar text-mode web browsers both share a `-dump` switch that outputs non-interactive text to STDOUT.  Both of them have a variety of other switches that can tweak the rendering of the text in a variety of ways.  The output from these two tools is similar, but the rest of your scripting will need to pay attention to the minor differences.  Each of these browsers will do a very good job of dumping 90% of web pages as text that is easy to process.  Of the problem 10% (a hand waving percentage, not a real measure), often one or the other tool will produce something reasonable to parse.  In certain cases, one of these browsers may produce useful results and the other will not.  Fortunately, it is easy simply to try both for a given task or site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the output from each tool against a portion of the HTTP response code page.  Obviously, I experimented to find the exact line ranges of output that would correspond.  You can see that only incidental formatting differences exist in this friendly HTML page. First with `lynx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          requester put on the request header fields.^[160][44]^[161][45]\n",
      "\n",
      "   413 Payload Too Large (RFC 7231)\n",
      "          The request is larger than the server is willing or able to\n",
      "          process. Previously called \"Request Entity Too Large\".^[162][46]\n",
      "\n",
      "   414 URI Too Long (RFC 7231)\n",
      "          The [163]URI provided was too long for the server to process.\n",
      "          Often the result of too much data being encoded as a\n",
      "          query-string of a GET request, in which case it should be\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "base='https://en.wikipedia.org/w/index.php?title='\n",
    "url=\"$base\"'List_of_HTTP_status_codes&oldid=947767948'\n",
    "lynx -dump $url | sed -n '397,406p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same part of the page again, but this time with `links`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           requester put on the request header fields.^[44]^[45]\n",
      "\n",
      "   413 Payload Too Large (RFC 7231)\n",
      "           The request is larger than the server is willing or able to\n",
      "           process. Previously called \"Request Entity Too Large\".^[46]\n",
      "\n",
      "   414 URI Too Long (RFC 7231)\n",
      "           The URI provided was too long for the server to process. Often the\n",
      "           result of too much data being encoded as a query-string of a GET\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "base='https://en.wikipedia.org/w/index.php?title='\n",
    "url=\"$base\"'List_of_HTTP_status_codes&oldid=947767948'\n",
    "links -dump $url | sed -n '377,385p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only differences here are one space difference in indentation of the definition element and some difference in the formatting of footnote links in the text.  In either case, it would be easy enough to define some rules for the patterns of terms and their definitions.  Something like this:\n",
    "\n",
    "* Look for a line that starts with 3 spaces followed by a 3 digit number;\n",
    "* Accumulate all non-blank lines following that, stop at blank line;\n",
    "* Strip the footnote/link markers from the texts;\n",
    "* Split the code number and text in the same manner as in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us wave goodbye to the Scylla of HTML, as we pass by, and fall into the Charybdis of PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portable Document Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This functionary grasped it in a perfect agony of joy, opened it with \n",
    "> a trembling hand, cast a rapid glance at its contents, and then, \n",
    "> scrambling and struggling to the door, rushed at length unceremoniously \n",
    "> from the room and from the house.<br/>–Edgar Allan Poe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepts**:\n",
    "\n",
    "* Identifying tabular regions\n",
    "* Extracting plain text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a great many commercial tools to extract data which has become hidden away in PDF (portable document format) files.  Unfortunately, many organizations—government, corporate, and others—issue reports in PDF format but do not provide data formats more easily accessible to computer analysis and abstraction.  This is common enough to have provided impetus for a cottage industry of tools for semi-automatically extracting data back out of these reports.  This book does not recommend use of proprietary tools about which there is no guarantee of maintenance and improvement over time; as well, of course, those tools cost money and are an impediment to cooperation among data scientists who work together on projects without necessarily residing in the same \"licensing zone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main elements that are likely to interest us in a PDF file.  An obvious one is tables of data, and those are often embedded in PDFs.  Otherwise, a PDF can often simply be treated as a custom text format, as we discuss in a section below.  Various kinds of lists, bullets, captions, or simply paragraph text, might have data of interest to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two open source tools I recommend for extraction of data from PDFs.  One of these it the command-line tool `pdftotext` which is part of the **Xpdf** and derived **Poppler** software suites.  The second is a Java tool called **tabula-java**.  Tabula-java is in turn the underlying engine for the GUI tool **Tabula**, and also has language bindings for Ruby (**tabula-extractor**), Python (**tabula-py**), R (**tabulizer**), and Node.js (**tabula-js**).  Tabula creates a small web server that allows interaction within a browser to do operations like creating lists of PDFs and selecting regions where tables are located.  The Python and R bindings also allow direct creation of data frames or arrays, with the R binding incorporating an optional graphical widget for region selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this discussion, we do not use any of the language bindings, nor the GUI tools.  For one-off selection of one-page data sets, the selection tools could be useful, but for automation of recurring document updates or families of similar documents, scripting is needed.  Moreover, while the various language bindings are perfectly suitable for scripting, we can be somewhat more language agnostic in this section by limiting ourselves to the command-line tool of the base library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example for this section, let us use a PDF that was output from the preface of this book itself.  There may have been small wording changes by the time you read this, and the exact formatting of the printed book or ebook will surely be somewhat different from this draft version.  However, this nicely illustrates tables rendered in several different styles that we can try to extract as data.  There are three tables, in particular, which we would like to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/preface-1.png\" alt=\"Preface page 5\" width=\"50%\"/>\n",
    "\n",
    "__Image: Page 5 of Book Preface__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On page 5 of the draft preface, a table is rendered by both Pandas and tibble, with corresponding minor presentation differences.  On page 7 another table is included that looks somewhat different again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/preface-2.png\" alt=\"Preface page 7\" width=\"50%\"/>\n",
    "\n",
    "__Image: Page 7 of Book Preface__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running tabula-java requires a rather long command line, so I have created a small bash script to wrap it on my personal system:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# script: tabula\n",
    "# Adjust for your personal system path\n",
    "TPATH='/home/dmertz/git/tabula-java/target'\n",
    "JAR='tabula-1.0.4-SNAPSHOT-jar-with-dependencies.jar'\n",
    "java -jar \"$TPATH/$JAR\" $@\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction will sometimes automatically recognize tables per page with the `--guess` option, but you can get better control by specifying a portion of a page where tabula-java should look for a table.  We simply output to STDOUT in the following code cells, but outputting to a file is just another option switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]:,,Last_Name,First_Name,Favorite_Color,Age\n",
      "\"\",Student_No,,,,\n",
      "\"\",1,Johnson,Mia,periwinkle,12.0\n",
      "\"\",2,Lopez,Liam,blue-green,13.0\n",
      "\"\",3,Lee,Isabella,<missing>,11.0\n",
      "\"\",4,Fisher,Mason,gray,NaN\n",
      "\"\",5,Gupta,Olivia,sepia,NaN\n",
      "\"\",6,Robinson,Sophia,blue,12.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tabula -g -t -p5 data/Preface-snapshot.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabula does a good, but not perfect, job. The Pandas style of setting the name of the index column below the other headers threw it off slightly.  There is also a spurious first column that is usually empty strings, but has a header as the output cell number.  However, these small defects are very easy to clean up, and we have a very nice CSV of the actual data in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from just above, however, that page 5 actually had *two tables* on it.  Tabula-java only captured the first one, which is not unreasonable, but is not all the data we might want.  Slightly more custom instructions (determined by moderate trial-and-error to determine the region of interest) can capture the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First\tLast\tAge\n",
      "<chr>\t<chr>\t\n",
      "bl>\t\t\n",
      "Mia\tJohnson\t12\n",
      "Liam\tLopez\t13\n",
      "Isabella\tLee\t11\n",
      "Mason\tFisher\tNaN\n",
      "Olivia\tGupta\tNaN\n",
      "Sophia\tRobinson\t12\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tabula -a'%72,13,90,100' -fTSV -p5 data/Preface-snapshot.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the output options, we chose tab-delimited rather than comma-separated for the output.  A JSON output is also available. Moreover, by adjusting the left margin (as percent, but as typographic points is also an option), we can eliminate the unecessary row numbers.  As before, the ingestion is good but not perfect.  The tibble formatting of data type markers is superfluous for us.  Discarding the two rows with unnecessary data is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for this example, let us capture the table on page 7 that does not have any of those data frame library extra markers.  This one is probably more typical of the tables you will encounter in real work.  For the example, we use points rather than page percentage to indicate the position of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number,Color,Number,Color\n",
      "1,beige,6,alabaster\n",
      "2,eggshell,7,sandcastle\n",
      "3,seafoam,8,chartreuse\n",
      "4,mint,9,sepia\n",
      "5,cream,10,lemon\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tabula -p7 -a'120,0,220,500' data/Preface-snapshot.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction here is perfect, although the table itself is less than ideal in that it it repeats the number/color pairs twice.  However, that is likewise easy enough to modify using data frame libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool tabula-java, as the name suggests, is only really useful for identifying tables.  In contrast, pdftotext creates a *best-effort* purely text version of a PDF.  Most of the time this is quite good.  From that, standard text processing and extraction techniques usually work well, including those that parse tables.  However, since an entire document (or a part of it selected by pages) is output, that lets us work with other elements like bullet lists, raw prose, or other identifiable data elements of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Missing data in the Favorite Color field should be substituted with\n",
      "the string <missing>.\n",
      "• Student ages should be between 9 and 14, and all other values are\n",
      "considered missing data.\n",
      "• Some colors are numerically coded, but should be dealiased. The\n",
      "mapping is:\n",
      "\n",
      "   Number     Color      Number    Color\n",
      "      1       beige          6     alabaster\n",
      "      2       eggshell       7     sandcastle\n",
      "      3       seafoam        8     chartreuse\n",
      "      4       mint           9     sepia\n",
      "      5       cream          10    lemon\n",
      "\n",
      "\n",
      "Using the small test data set is a good way to test your code. But try\n",
      "also manually adding more\n",
      "rows with similar, or different, problems in them, and see how well your\n",
      "code produces a reasonable\n",
      "result.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Start with page 7, tool writes to .txt file \n",
    "# Use layout mode to preserve horizontal position\n",
    "pdftotext -f 7 -layout data/Preface-snapshot.pdf\n",
    "# Remove 25 spaces from start of lines\n",
    "# Wrap other lines that are too wide\n",
    "sed -E 's/^ {,25}//' data/Preface-snapshot.txt |\n",
    "    fmt -s | \n",
    "    head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabular part in the middle would be simple to read as a fixed width format.  The bullets at top or the paragraph at bottom might be useful for other data extraction purposes.  In any case, it is plain text at this point, which is easy to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us turn now to analyzing images, mostly for their metadata and overall statistical characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As the Chinese say, 1001 words is worth more than a picture.<br/>–John McCarthy<sup><i>picture</i></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"picture\"\n",
    "     style=\"display: inline-block; margin: 0 5% 0 5%; border-style: solid; border-width: 1px\">\n",
    "     <i>picture</i><br/>\n",
    "The quote McCarthy plays off of is not, of course, of ancient Chinese origin.  Like much early 20th century American sinophilia—inevitably tinged with sinophobia—it originated with an advertising agency.  Henrik Ibsen had said \"A thousand words leave not the same deep impression as does a single deed\" prior to his 1906 death.  This was adapted in March 1911, by Arthur Brisbane speaking to the Syracuse Advertising Men's Club, as \"Use a picture. It's worth a thousand words.\" Later repetitions added the alleged source as a \"Chinese proverb\" or even a false attribution to Confucius specifically, presumably to lend credence to the slogan.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepts**:\n",
    "\n",
    "* OCR and image recognition (outside scope)\n",
    "* Color models\n",
    "* Pixel statistics\n",
    "* Channel preprocessing\n",
    "* Image metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For certain purposes, raster images are themselves the data sets of interest to us.  \"Raster\" just means rectangular collections of pixel values. The field of machine learning around image recognition and image processing is far outside the scope of this book.  The few techniques in this section might be useful to get your data ready to the point of developing input to those tools, but no further than that.  Also not considered in this book are other kinds of recognition of the *content* of images at a high-level.  For example, optical character recognition (OCR) tools might recognize an image as containing various strings and numbers as rendered fonts, and those values might be the data we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the misfortune of having data that is only available in printed and scanned form, you most certainly have my deep sympathy.  Scanning the images using OCR is likely to produce noisy results with many misrecognitions.  Detecting those is addressed in chapter 4 (*Anomaly Detection*); essentially you will get either wrong strings or wrong numbers when these errors happen, ideally the errors will be identifiable.  However, the specifics of those technologies are not within the current scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we merely want to present tools to read in images as numeric arrays, and perform a few basic processing steps that might be used in your downstream data analysis or modeling.  Within Python, the libary **Pillow** is the go-to tool (backward compatible successor to **PIL**, which is deprecated).  Within R, the **imager** library seems to be most widely used for the general purpose tasks of this section.  As a first task, let us examine and describe the raster images used in the creation of this book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "\n",
    "for fname in glob('img/*'):\n",
    "    try:\n",
    "        with Image.open(fname) as im:\n",
    "            print(fname, im.format, \"%dx%d\" % im.size, im.mode)\n",
    "    except IOError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that mostly PNG images were used, with a smaller number of JPEGs.  Each has certain spatial dimensions, by width then height, and each is either RGB, or RGBA if it includes an alpha channel.  Other images might be HSV format.  Converting between color spaces is easy enough using tools like Pillow and imager, but it is important to be aware of which model a given image uses.  Let us read one in, this time using R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(imager)\n",
    "confucius <- load.image(\"img/Konfuzius-1770.jpg\")\n",
    "print(confucius)\n",
    "plot(confucius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyze the contours of the pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can work on getting a feel for the data, which at heart is simply an array of values, with some tools the library provides.  In the case of imager which is built on **CImg**, the internal representation is 4-dimensional.  Each plane is  an X by Y grid of pixels (left-to-right, top-to-bottom).  However, the format can represent a stack of images—for example, an animation—in the depth dimension.  The several color channels (if the image is not grayscale) are the final dimension of the array.  The Confucius example is a single image, so the third dimension is of length one.  Let us look at some summary data about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "grayscale(confucius) %>% \n",
    "    hist(main=\"Luminance values in Confucius drawing\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Save histogram to disk\n",
    "png(\"img/(Ch03)Luminance values in Confucius drawing.png\", width=1200)\n",
    "grayscale(confucius) %>% \n",
    "    hist(main=\"Luminance values in Confucius drawing\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we would like to look at the distribution only of one color channel instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "B(confucius) %>% \n",
    "    hist(main=\"Blue values in Confucius drawing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Save histogram to disk\n",
    "png(\"img/(Ch03)Blue values in Confucius drawing.png\", width=1200)\n",
    "B(confucius) %>% \n",
    "    hist(main=\"Blue values in Confucius drawing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above simply utilize the standard R histogram function.  There is nothing special about the fact that the data represents an image.  We could perform whatever statistical tests or summarizations we wanted on the data to make sure it *makes sense* for our purpose; a histogram is only a simple example to show the concept.  We can also easily transform the data into a tidy data frame.  As of this writing, there is an \"impedance error\" in converting directly to a tibble, so the below cell uses an intermediate data.frame format. Tibbles are *often* but not *always* drop in replacements when functions were written to work with data.frame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "data <- as.data.frame(confucius) %>%\n",
    "    as_tibble %>%\n",
    "    # channels 1, 2, 3 (RGB) as factor\n",
    "    mutate(cc = as.factor(cc))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Python and PIL/Pillow, working with image data is very similar.  As in R, the image is an array of pixel values with some metadata attached to it.  Just for fun, we use a variable name with Chinese characters to illustrate that such is supported in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courtesy name: Zhòngní (仲尼)\n",
    "# \"Kǒng Fūzǐ\" (孔夫子) was coined by 16th century Jesuits\n",
    "仲尼 = Image.open('img/Konfuzius-1770.jpg')\n",
    "data = np.array(仲尼)\n",
    "print(\"Image shape:\", data.shape)\n",
    "print(\"Some values\\n\", data[:2, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Pillow format, images are stored as 8-bit unsigned integers rather than as floating-point numbers in [0.0, 1.0] range.  Converting between these is easy enough, of course, as is other normalization.  For example, for many neural network tasks, the prefered representation is values centered at zero with standard deviation of one.  The array used to hold Pillow images in 3-dimensional since it does not have provision for stacking multiple images in the same object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful to perform manipulation of image data before processing.  The below example is contrived, and similar to one used in the library tutorial.  The idea in the next few code lines is that we will mask the image based on the values in the blue channel, but then use that to selectively zero-out red values.  The result is not visually attractive for a painting, but one can imagine it might be useful for e.g. medical imaging or false-color radio astronomy images (I am also working around making a transformation that is easily visible in a monochrome book as well as in full color)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convention used in the `.paste()` method is a bit odd.  The rule is: Where the mask is 255, copied as is; where mask is 0, preserve current value (blend if intermediate). The effect overall in the color version is that in the mostly red-tinged image, the greens dominate at the edges where the image had been most red.  In grayscale it mostly just darkens the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the Confucius image into individual bands\n",
    "source = 仲尼.split()\n",
    "R, G, B = 0, 1, 2\n",
    "\n",
    "# select regions where blue is less than 100\n",
    "mask = source[B].point(lambda i: 255 if i < 100 else 0)\n",
    "source[R].paste(0, None, mask)\n",
    "im = Image.merge(仲尼.mode, source)\n",
    "im.save('img/(Ch03)Konfuzius-bluefilter.jpg')\n",
    "ImageOps.scale(im, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageOps.scale(仲尼, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example we mentioned is that transformation of the color space might be useful.  For example, rather than look at colors red, green, and blue, it might be that hue, saturation, and lightness are better features for your modeling needs.  This is a deterministic transformation of the data, but emphasizing different aspects.  It is something analogous to the decompositions like principal component analysis that is discussed in chapter 7 (*Feature Engineering*).  Here we convert from an RGB to HSL representation of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "confucius.hsv <- RGBtoHSL(confucius)\n",
    "data <- as.data.frame(confucius.hsv) %>%\n",
    "    as_tibble %>%\n",
    "    # channels 1, 2, 3 (HSV) as factor\n",
    "    mutate(cc = as.factor(cc))\n",
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the individual values and the shape of the space have changed in this transformation.  The transformation is lossless, beyond minor rounding issues.  A summary by channel will illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "data %>% \n",
    "    mutate(cc = recode(\n",
    "        cc, `1`=\"Hue\", `2`=\"Saturation\", `3`=\"Value\")) %>%\n",
    "    group_by(cc) %>%\n",
    "    summarize(Mean = mean(value), SD = sd(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at perhaps the most important aspect of images to data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photographic images may contain metadata embedded inside them.  Specifically, the *Exchangeable Image File Format* (Exif) specifies how such metadata can be embedded in JPEG, TIFF, and WAV formats (the last is an audio format).  Digital cameras typically add this information to the images they create, often including details such as timestamp and latitude/longitude location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data fields within an Exif mapping are textual, numeric, or tuples; others are binary data.  Moreover, the *keys* in the mapping are from ID numbers that are not meaningful to humans directly; this mapping is a published standard, but some equipment makers may introduce their own IDs as well.  The binary fields contain a variety of types of data, encoded in various ways.  For example, some cameras may attach small preview images as Exif metadata; but simpler fields are also encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will utilize Pillow to return two dictionaries, one for the textual data, the other for the binary data.  Tag IDs are expanded to human readable names, where available.  Pillow uses \"camel case\" for these names, but other tools have different variations on capitalization and punctuation within the tag names.  The casing by Pillow is what I like to call Bactrian case—as opposed to Dromedary case—both of which differ from Python's usual \"snake case\" (e.g. `BactrianCase` versus `dromedaryCase` versus `snake_case`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.ExifTags import TAGS\n",
    "\n",
    "def get_exif(img):\n",
    "    txtdata, bindata = dict(), dict()\n",
    "    for tag_id in (exifdata := img.getexif()):\n",
    "        # Lookup tag name from tag_id if available\n",
    "        tag = TAGS.get(tag_id, tag_id)\n",
    "        data = exifdata.get(tag_id)\n",
    "        if isinstance(data, bytes):\n",
    "            bindata[tag] = data\n",
    "        else:\n",
    "            txtdata[tag] = data\n",
    "    return txtdata, bindata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether the Confucius image has any metadata attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_exif(仲尼)  # Zhòngní, i.e. Confucius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this image does not have any such metadata.  Let us look instead at a photograph taken of the author next to a Lenin statue in Minsk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could continue using multi-lingual variable names by\n",
    "# choosing `Ленин`, `Ульянов` or `Мінск`\n",
    "dqm = Image.open('img/DQM-with-Lenin-Minsk.jpg')\n",
    "ImageOps.scale(dqm, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image, taken with a digital camera, indeed has Exif metadata.  These generally concern photographic settings, which are perhaps valuable to analyze in comparing images.  This example also has a timestamp, although not in this case a latitude/longitude position (the camera used did not have a GPS sensor).  Location data, where available, can obviously be valuable for many purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtdata, bindata = get_exif(dqm)\n",
    "txtdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail we notice in the textual data is that the tag ID 34864 was not unaliased by Pillow.  I can locate external documentation indicating that the ID should indicate \"Exif.Photo.SensitivityType\" but Pillow is apparently unaware of that ID.  The bytes strings may contain data you wish to utilize, but the meaning given to each field is different and must be compared to reference definitions.  For example, the field `ExifVersion` is defined as ASCII bytes, but *not* as UTF-8 encoded bytes like regular text field values.  We can view that using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindata['ExifVersion'].decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the tag named `ComponentsConfiguration` consists of four bytes, with each byte representing a color code.  The function `get_exif()` produces separate text and binary dictionaries (`txtdata` and `bindata`). Let us decode `bindata` with a new special function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components(cc):\n",
    "    colors = {0: None,\n",
    "              1: 'Y', 2: 'Cb', 3: 'Cr',\n",
    "              4: 'R', 5: 'G', 6: 'B'}\n",
    "    return [colors.get(c, 'reserved') for c in cc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components(bindata['ComponentsConfiguration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other binary fields are encoded in other ways.  The specifications are maintained by the Japan Electronic Industries Development Association (JEIDA).  This section intends only to give you a feel for working with this kind of metadata, and is by no means a complete reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us turn our attention now to the specialize binary data formats we sometimes need to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Serialized Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I usually solve problems by letting them devour me.<br/>–Franz Kafka "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepts**:\n",
    "\n",
    "* Prefer existing libraries\n",
    "* Bytes and struct data types\n",
    "* Offset layout of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a great many binary formats that data might live in.  Everything very popular has grown good open source libraries, but you may encounter some legacy or in-house format for which this is not true.  Good general advice is that unless there is an ongoing and/or performance sensitive need for processing an unusual format, try to leverage existing parsers.  Custom formats can be tricky, and if one is uncommon, it is as likely as not also to be underdocumented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an existing tool is only available in a language you do not wish to use for your main data science work, nonetheless see if that can be easily leveraged to act only as as a means to export to a more easily accessed format.  A fire-and-forget tool might be all you need, even if it is one that runs recurringly, but asynchronously with the actual data processing you need to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, let as assume that the optimistic situation is not realized, and we have nothing beyond some bytes on disk, and some possibly flawed documentation to work with.  Writing the custom code is much more the job of a systems engineer than a data scientist; but we data scientists need to be polymaths, and we should not be daunted by writing a little bit of systems code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this relatively short section, we look at a simple and straightforward binary format.  Moreover, this is a real-world data format for which we do not actually need a custom parser.  Having an actual well-tested, performant, and bullet-proof parser to compare our toy code with is a good way to make sure we do the right thing.  Specifically, we will read data stored in the [NumPy NPY format](https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.format.html#module-numpy.lib.format), which is documented as follows (abridged):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first 6 bytes are a magic string: exactly `\\x93NUMPY`.\n",
    "* The next 1 byte is an unsigned byte: the major version number of the file format, e.g. `\\x01`.\n",
    "* The next 1 byte is an unsigned byte: the minor version number of the file format, e.g. `\\x00`. \n",
    "* The next 2 bytes form a little-endian unsigned short int: the length of the header data HEADER_LEN.\n",
    "* The next HEADER_LEN bytes are an ASCII string which contains a Python literal expression of a dictionary.\n",
    "* Following the header comes the array data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read in some binary data using the standard reader, using Python and NumPy, to understand what type of object we are trying to reconstruct.  It turns out that the serialization was of a 3-dimensional array of 64-bit floating-point values.  A small size was chosen for this section, but of course real-world data will generally be much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(open('data/binary-3d.npy', 'rb'))\n",
    "print(arr, '\\n', arr.shape, arr.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually examining the bytes is a good way to have a better feel for what is going on with the data.  NumPy is, of course, a clearly and correctly documented project; but for some hypothetical format, this is an opportunity to potentially identify problems with the documentation not matching the actual bytes.  More subtle issues may arise in the more detailed parsing; for example, the meaning of bytes in a particular location can be contingent on flags occurring elsewhere.  Data science is, in surprisingly large part, a matter of eyeballing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hexdump -Cv data/binary-3d.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let us make sure the file really does match the type we expect in having the correct \"magic string.\"  Many kinds of files are identified by a characteristic and distinctive first few bytes.  In fact, the common utility on Unix-like systems, `file` uses exactly this knowledge via a database describing many file types.  For a hypothetical rare file type (i.e. not NumPy), this utility may not know about the format; nonetheless, the file might still have such a header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file data/binary-3d.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, let us open a file handle for the file, and proceed with trying to parse it according to its specification.  For this, in Python, we will simply open the file in bytes mode, so as not to convert to text, and read various segments of the file to verify or process portions.  For this format, we will be able to process it strictly sequentially, but in other cases it might be necessary to seek to particular byte positions within the file.  The Python `struct` module will allow us to parse basic numeric types from bytestrings.  The `ast` module will let us create Python data structures from raw strings without a security risk that `eval()` can encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct, ast\n",
    "binfile = open('data/binary-3d.npy', 'rb')\n",
    "\n",
    "# Check that the magic header is correct\n",
    "if binfile.read(6) == b'\\x93NUMPY':\n",
    "    vermajor = ord(binfile.read(1))\n",
    "    verminor = ord(binfile.read(1))\n",
    "    print(f\"Data appears to be NPY format, \"\n",
    "          f\"version {vermajor}.{verminor}\")\n",
    "else:\n",
    "    print(\"Data in unsupported file format\")\n",
    "    print(\"*** ABORT PROCESSING ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to determine how long the header is, and then read it in.  The header is always ASCII in NPY version 1, but may be UTF-8 in version 3.  Since ASCII is a subset of UTF-8, decoding does no harm even if we do not check the  version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little-endian short int (tuple 0 element)\n",
    "header_len = struct.unpack('<H', binfile.read(2))[0]\n",
    "# Read specified number of bytes\n",
    "# Use safer ast.literal_eval()\n",
    "header = binfile.read(header_len)\n",
    "# Convert header bytes to a dictionary\n",
    "header_dict = ast.literal_eval(header.decode('utf-8'))\n",
    "print(f\"Read {header_len} bytes \"\n",
    "      f\"into dictionary: \\n{header_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this dictionary stored in the header gives a nice description of the dtype, value order, and the shape, the convention used by NumPy for value types is different from that used in the `struct` module.  We can define a (partial) mapping to obtain the correct spelling of the data type for the reader.  We only define this mapping for some data types encoded as *little-endian*, but the *big-endian* versions would simply have a greater-than sign instead.  The key for 'fortran_order' indicates whether the fastest or slowest varying dimension is contiguous in memory.  Most systems use \"C order\" instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not aiming for high-efficiency here, but in minimizing code.  Therefore, I will expediently read the actual data into a simple list of values first, then later convert that to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spelling of data types and find the struct code\n",
    "dtype_map = {'<i2': '<i', '<i4': '<l', '<i8': '<q',\n",
    "             '<f2': '<e', '<f4': '<f', '<f8': '<d'}\n",
    "dtype = header_dict['descr']\n",
    "fcode = dtype_map[dtype]\n",
    "# Determine number of bytes from dtype spec\n",
    "nbytes = int(dtype[2:])\n",
    "\n",
    "# List to hold values\n",
    "values = []\n",
    "\n",
    "# Python 3.8+ \"walrus operator\"\n",
    "while val_bytes := binfile.read(nbytes):\n",
    "    values.append(struct.unpack(fcode, val_bytes)[0])\n",
    "    \n",
    "print(\"Values:\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the raw values into an actual NumPy array of appropriate shape and dtype now.  We also will look for whether to use Fortran- or C-order in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = header_dict['shape']\n",
    "order = 'F' if header_dict['fortran_order'] else 'C'\n",
    "newarr = np.array(values, dtype=dtype, order=order)\n",
    "newarr = newarr.reshape(shape)\n",
    "print(newarr, '\\n', newarr.shape, newarr.dtype)\n",
    "print(\"\\nMatched standard parser:\", (arr == newarr).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as binary data can be oddball, so can text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Text Formats\n",
    "\n",
    "> Need we emphasize the similarity of these two sequences? Yes, for the \n",
    "> resemblance we have in mind is not a simple collection of traits chosen\n",
    "> only in order to delete their difference. And it would not be enough to \n",
    "> retain those common traits at the expense of the others for the slightest \n",
    "> truth to result. It is rather the intersubjectivity in which the two \n",
    "> actions are motivated that we wish to bring into relief, as well as the \n",
    "> three terms through which it structures them.<br/>–Jacques Lacan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepts**:\n",
    "\n",
    "* Line-oriented and hierarchical structures\n",
    "* Heuristics to identify data of interest\n",
    "* Character encodings and mojibake\n",
    "* Guessing with chardet (character detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In life as a data scientist—but especially if you occasionally wear the hat of a systems administrator or similar role, you will encounter textual data with unusual formats.  Log files are one common source of these kinds of files.  Many or most log files *do* stick to the record-per-line convention; if so, we are given an easy way to separate records.  From there, a variety of rules or heuristics can be used to determine exactly what *kind* of record the line corresponds to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all log files, however, stick to a line convention.  Moreover, over time, you will likewise encounter other types of files produced by tools that store nested data and chose to create their own format rather than use some widely used standard.  For hierarchical or other non-tabular structures the motivation for eschewing strict record-per-line format is often compelling and obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, the authors of the programs creating one-off formats are entirely free of blame.  Standard formats for representing non-tabular data did not exist a decade prior to this writing in 2020, or at least were not widely adopted across a range of programming languages in that not-so-distant past.  Depending on your exact domain, legacy data and formats are likely to dominate your work.  For example, JSON was first standardized in 2013, as ECMA-404.  YAML was created in 2001, but not widely used before approximately 2010.  XML dates to 1996, but has remained unwieldy for human-readable formats since then.  Hence many programmers have gone their own way, and left as traces the files you now need to import, analyze, and process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Structured Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scanning my own system, I found a good example of a reasonably human-readable log file that is not parsable in a line-oriented manner.  The Perl package management tool `cpan` logs the installation actions of each library it manages.  The format used for such logs varies per package (very much in a Perl style).  The package *Archive::Zip* left the discussed log on my system (for its self-tests).  This data file contains sections that are actual Perl code defining test classes, interspersed with unformatted output messages.  Each of the classes has a variety of attributes, largely overlapping but not identical.  A sensible memory data format for this is a data frame with mising values marked where a given attribute name does not exist for a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we could use Perl itself to process those class definitions. However, that is unlikely to be the programming language we wish to use actually to work with the data extracted.  We will use Python to read the format, and use only heuristics about what elements we expect.  Notably, we **cannot** statically parse Perl, which task was shown to be strictly equivalent to solving the *[halting problem](https://en.wikipedia.org/wiki/Halting_problem)* by Jeffrey Kegler in several 2008 essays for [The Perl Review](http://www.jeffreykegler.com/Home/perl-and-undecidability).  Nonetheless, the output in our example uses a friendly, but not formally defined, subset of the Perl language.  Here is a bit of the file being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -25 data/archive-zip.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer science theory to the side, we can notice some patterns in the file that will suffice for us.  Every *record* that we care about starts a line with a dollar sign, which is the marker used for variable names in Perl and some other languages.  That line also happens to follow with the class constructor `bless()`.  We find the end of the record by a line ending with `);`.  On that same last line, we also find the name of the class being defined, but we do not, in this example, wish to retain the common prefix `Archive::Zip::` that they all use.  Also stipulated for this example is that we will not try to process any additional data that is contained in the output lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly it would be possible to create a valid construction of a Perl class that our heuristic rules will fail to capture accurately.  But our goal here is not to implement the Perl language, but only to parse the very small subset of it contained in this particular file (and hopefully cover a family of similar logs that may exist for other CPAN libraries). A small state machine is constructed to branch within a loop over lines of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cpan_log(fh):\n",
    "    \"Take a file-like object, produce a DF of classes generated\"\n",
    "    import pandas as pd\n",
    "    # Python dictionaries are ordered in 3.6+\n",
    "    classes = {}\n",
    "    in_class = False\n",
    "    \n",
    "    for n, line in enumerate(fh):\n",
    "        # Remove surrounding whitespace\n",
    "        line = line.strip()\n",
    "        # Is this a new definition?\n",
    "        if line.startswith('$'):\n",
    "            new_rec = {}\n",
    "            in_class = True  # One or more variables contain the \"state\"\n",
    "            \n",
    "        # Is this the end of the definition?\n",
    "        elif line.endswith(');'):\n",
    "            # Possibly fragile assumption of parts of line\n",
    "            _, classname, _ = line.split()\n",
    "            barename = classname.replace('Archive::Zip::', '')\n",
    "            # Just removing extra quotes this way\n",
    "            name = ast.literal_eval(barename)\n",
    "            # Distinguish entries with same name by line number\n",
    "            classes[f\"{name}_{n}\"] = new_rec\n",
    "            in_class = False\n",
    "            \n",
    "        # We are still finding new key/val pairs\n",
    "        elif in_class:\n",
    "            # Split around Perl map operator\n",
    "            key, val = [s.strip() for s in line.split('=>')]\n",
    "            # No trailing comma, if it was present\n",
    "            val = val.rstrip(',')\n",
    "            # Special null value needs to be translated\n",
    "            val = \"None\" if val == \"undef\" else val\n",
    "            # Also, just quote variables in vals\n",
    "            val = f'\"{val}\"' if val.startswith(\"$\") else val\n",
    "            # Safe evaluate strings to Python objects\n",
    "            key = ast.literal_eval(key)\n",
    "            val = ast.literal_eval(val)\n",
    "            # Add to record dictionary\n",
    "            new_rec[key] = val\n",
    "            \n",
    "    return pd.DataFrame(classes).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined is a bit longer than most examples in this book, but is typical of a small text processing function.  The use of the state variable `in_class` is common when various lines may belong to one domain of parsing or another.  This pattern of looking for a start state based on something about a line, accumulating contents, then looking for a stop state based on a different line property, is very common in these kinds of tasks.  Beyond the state maintenance, the rest of the lines are, in the main, merely some minor string manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let as read and parse the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_cpan_log(open('data/archive-zip.log'))\n",
    "df.iloc[:, [4, 11, 26, 35]]  # Show only few columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the DataFrame might better be utilized as a Series with a hierarchical index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with show_more_rows(25):\n",
    "    print(df.unstack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question of character encodings of text formats is somewhat orthogonal to the data issues the bulk of this book addresses.  However, being able to read the content of a text file is an essential step in processing the data within it, so we should look at possible problems.  The problems that occur are an issue for \"legacy encodings\" but should be solved as text formats standardize on Unicode.  That said, it is not uncommon that you need to deal with files that are decades old, either preceding Unicode altogether, or created before organizations and software (such as operating systems) fully standardized their text formats to Unicode.  We will look both at the problems that arise and heuristic tools to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The American Standard Code for Information Interchange(ASCII) was created in the 1960s as a standard for encoding text data.  However, at the time, in the United States, consideration was only made to encode the characters used in English text.  This included upper and lowercase characters, some basic punctuation, numerals, and a few other special or control characters (such as newline, the terminal bell, etc).  To accommodate this collection of symbols, 128 positions were sufficient, so the ASCII standard defines only values for 8-bit bytes where the *high-order bit* is a zero.  Any byte with a high-order bit set to one is not an ASCII character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the ASCII standard in a \"compatible\" way are the ISO-8859 character encodings.  These were developed to cover the characters in (approximately) phonemic alphabets, primarily those originating in Europe.  Many alphabetic languages are based on Roman letters, but add a variety of diacritics that are not used in English.  Other alphabets are of moderate size, but unrelated to English in letter forms, such as Cyrillic, Greek, and Hebrew.  All of the encodings that make up the ISO-8859 family preserve the low-order values of ASCII, but encode additional characters using the high-order bits of each byte.  The problem is that 128 additional values (in a byte with 256 total values) is not large enough to accommodate all of those different extra characters, so particular members of the family (e.g. ISO-8859-6 for Arabic) use the high-order bit values in incompatible ways.  This allows English text to be represented in all encodings in this family, but each sibling is mutually incompatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CJK languages (Chinese-Japanese-Korean) the number of characters needed is vastly larger than 256, so any single byte encoding is not suitable to represent these languages.  Most encodings that were created for these languages use 2-bytes for each character, but some are variable length.  However, a great many incompatible encodings were created, not only for the different languages, but also within a particular language.  For example, EUC-JP, SHIFT_JIS, and ISO-2022-JP, are all encodings used to represent Japanese text, in mutually incompatible ways.  Abugida writing systems, such as Devanagari, Telugu, or Geʽez represent syllables, and hence have larger character sets than alphabetic systems; however, most do not utilize letter case, hence roughly halving the code points needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding to the historical confusion, not only do other encodings outside of the ISO-8859 family exist for alphabetic languages (including some also covered by an ISO-8859 member), but Microsoft in the 1980s fervently pursued its \"embrace-extend-extinguish\" strategy to try to kill open standards.  In particular, the windows-12NN character encodings are deliberately \"almost-but-not-quite\" the same as corresponding ISO-8859 encodings.  For example windows-1252 uses most of the same code points as ISO-8859-1, but is just enough different not to be entirely compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sometimes amusing, but usually frustrating, result of trying to decode a byte sequence using the wrong encoding is called mojibake (meaning \"character transformation\" in Japanese, or more holistically \"corrupted text\").  Depending on the pairs of encoding used for writing and reading, the text may superficially resemble genuine text, or it might have displayed markers for unavailable characters and/or punctuation symbols that are clearly misplaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode is a specification of code points for all characters in all human languages.  It may be *encoded* as bytes in multiple ways. However, if a format other than the default and prevalent UTF-8 is used, the file will always have a \"magic number\" at its start, and the first few bytes will unambiguously encode the byte-length and endianness of the encoding.  UTF-8 files are neither required nor encouraged to use a byte-order mark (BOM), but one exists that is not ambiguous with any code points.  UTF-8 itself is a variable length encoding; all ASCII characters remain encoded as a single byte, but for other characters, special values that use the high-order bit trigger an expectation to read additional bytes to decide what Unicode character is encoded. For the data scientist, it is enough to know that all modern programming languages and tools handle Unicode files seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few short texts are snippets of Wikipedia articles on character encoding written for various languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in glob('data/character-encoding-*.txt'):\n",
    "    bname = os.path.basename(fname)\n",
    "    try:\n",
    "        open(fname).read()\n",
    "        print(\"Read 'successfully':\", bname, \"\\n\")\n",
    "    except Exception as err:\n",
    "        print(\"Error in\", bname)\n",
    "        print(err, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something goes wrong with trying to read the text in these files.  If we are so fortunate as to know the encoding used, it is easy to remedy the issue.  However, the files themselves do not record their encoding.  As well, depending on what fonts you are using for display, some characters may show as boxes or question marks on your screen, which makes identification of the problems harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_file = 'data/character-encoding-zh.txt'\n",
    "print(open(zh_file, encoding='GB18030').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we take a hint from the filename that the encoding represents Chinese text, we will either fail or get mojibake as a result if we use the wrong encoding in our attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wrong Chinese encoding\n",
    "    open(zh_file, encoding='GB2312').read()\n",
    "except Exception as err:\n",
    "    print(\"Error in\", os.path.basename(zh_file))\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not see the error immediately.  If we had only read 11 bytes, it would have been \"valid\" (but the wrong characters).  Likewise, the file `character-encoding-nb.txt` above would have succeeded for an entire 170 bytes without having a problem.  We can see a wrong guess going wrong in these files.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_file = 'data/character-encoding-ru.txt'\n",
    "print(open(ru_file, encoding='iso-8859-10').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read *something*, but even without necessarily knowing any of the languages at issue, it is fairly clearly gibberish.  As readers of English, we can at least recognize the base letters that these mostly diacritic forms derive from.  They are jumbled together in a manner that doesn't follow any real sensible phonetic rules, such as vowels and consonants roughly alternating, or a meaningful capitalization pattern.  Included here is the brief English phrase \"character set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, the text genuinely is in the ISO-8859 family, but we chose the wrong sibling among them.  This gives us one type of mojibake.  As the filename hints at, this happens to be in Russian, and uses the Cyrillic member of the ISO-8859 family.  Readers may not know the Cyrillic letters, but if you have seen any signage or text incidentally, this text will not look *obviously wrong.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(ru_file, encoding='iso-8859-5').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if you have seen writing in Greek, this version will perhaps not look obviously wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_file = 'data/character-encoding-el.txt'\n",
    "print(open(el_file, encoding='iso-8859-7').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merely being not obviously wrong in a language you are not familiar with is a weak standard to meet.  Having native, or at least modestly proficient, readers of the languages at question will help, if that is possible.  If this is not possible—which often it will not be if you are processing many files with many encodings—automated tools can make reasonable heuristic guesses.  This does not guarantee correctness, but it is suggestive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the Python **chardet** module works is similar to the code in all modern web browsers.  HTML pages can declare their encoding in their headers, but this declaration is often wrong, for various reasons.  Browsers do some hand holding and try to make better guesses when the data clearly does not match declared encoding.  The general idea in this detection is threefold.  A detector will scan through multiple candidate encodings to reach a best guess.\n",
    "\n",
    "* Under the candidate encoding, are any of the byte values or sequences simply invalid?\n",
    "* Under the candidate encoding, is the character frequency similar to that typically encountered in the language(s) often encoded using that encoding?\n",
    "* Under the candidate encoding, are digraph frequencies similar to those typically encountered?\n",
    "\n",
    "We do not need to worry about the exact details of the probability ranking, just the API to use.  Implementations of the same algorithm are available in a variety of programming languages. Let us look at the guesses `chardet` makes for some of our text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "for fname in glob('data/character-encoding-*.txt'):\n",
    "    # Read the file in binary mode\n",
    "    bname = os.path.basename(fname)\n",
    "    raw =  open(fname, 'rb').read()\n",
    "    print(f\"{bname} (best guess):\")\n",
    "    guess = chardet.detect(raw)\n",
    "    print(f\"    encoding: {guess['encoding']}\")\n",
    "    print(f\"  confidence: {guess['confidence']}\")\n",
    "    print(f\"    language: {guess['language']}\")\n",
    "    print()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These guesses are only partially correct.  The language code `nb` is actually Norwegian Bokmål, not Turkish.  This guess has a notably lower probability than others.  Moreover, it was actually encoded using ISO-8859-10.  However, in this particular text, all characters are identical between ISO-8859-9 and ISO-8859-10, so that aspect is not really wrong.  A larger text would more reliably guess between Bokmål and Turkish by letter and digram frequency; it does not make much difference if that is correct for most purposes, since our concern as data scientists is to get the *data* correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open('data/character-encoding-nb.txt', \n",
    "           encoding='iso-8859-9').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guess about the `zh` text is wrong as well.  We have already tried reading that file as GB2312 and reached an explicit failure in doing so. This is where domain knowledge becomes relevant.  GB18030 is strictly a superset of GB2312.  In principle, the Python chardet module is aware of GB18030, so the problem is not a missing feature per se.  Nonetheless, in this case, unfortunately, chardet guesses an impossible encoding, in which one or more encoded characters do not exist in the subset encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors in encoding inference are illustrative, even if not too serious in these particular cases.  Adding more text than 2-3 sentences would make guesses more reliable, and most text documents will be much longer.  However, text formats for non-text data will typically only have short snippets of text, often just single words in a categorical feature. The specific strings 'blue', 'mavi', 'blå', 'blau', and 'sininen' are all plausibe words in English, Turkish, Norwegian, German, and Finnish.  The a-ring character does not occur in Turkish or English, but other than that, the distinction is stictly in vocabulary not letter or digraph plausibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a CSV file with personal names will only have clusters of 5-10 letters for each name, not full paragraphs.  The number of letters and digraphs is small, and even if uncommon ones occur in isolation, that is hardly definitive.  If you have some domain knowlege or guidance on the problem, you could write more custom code to validate candidate encodings against language-specific wordlists (including common names); even there, you would have to allow a certain rate of non-matches for misspellings and rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here two exercises.  One of them deals with a custom binary format, the other with web scraping.  Not every topic of this chapter is addressed in the exercises, but these two are important domains for practical data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing the NPY Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary data we read from the NPY was in the simplest format we could choose.  For this exercise you want to process a somewhat more complex binary file using your own code.  Write a custom function that reads a file into a NumPy array, and test it against several arrays you have serialized using `numpy.save()` or `numpy.savez()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cases for your function are at the URLs:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/students.npy\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/students.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not previously looked at the NPZ format, but it is a zip archive of one or more NPY files, allowing both compression and storage of multiple arrays.  Ideally your function will handle both formats, and will determine which type of file you are reading based on the magic string in the first few bytes.  As a first pass, only try to parse the NPY version, then enhance from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the official readers, we can see that this array adds something the earlier example had not.  Specifically, it stores a `recarray` that combines several data types into each value in the array.  The rules we described earlier in this chapter will actually still suffice, but you have to think about them carefully.  The data we want to match in your reader will be exactly the same as using the official reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = np.load(open('data/students.npy', 'rb'))\n",
    "print(students)\n",
    "print(\"\\nDtype:\", students.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you move on to processing the NPZ format, you can compare again with the official reader.  As mentioned, this might have several arrays inside it, although only one is stored in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = np.load(open('data/students.npz', 'rb'))\n",
    "print(arrs)\n",
    "arrs.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of `arr_0` within the NPZ file is identical to the single array in the NPY.  However, after you have successfully parsed this NPZ file, try creating one or more others that actually do store multiple arrays, and parse those using custom code.  Decide on the best API to use for a function that may need to return either one or several arrays.  For this part of the task, the Python standard library module `zipfile` will be very helpful for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no reason this exercise has to be performed in Python.  Other programming languages are perfectly well able to read binary data, and the general steps involved will be very similar to those performed in this chapter in the Binary Serialized Data Structures section.  You could, for example, read the data within an NPY file into an R array instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Web Traffic\n",
    "\n",
    "The author's web domain, gnosis.cx, has been operating for more than two decades, and retains most of the \"Web 0.5\" technology and visual style it was first authored with.  One thing the web host provides, as do most others, is reports on traffic at the site (using nearly as ancient styling as that of the domain itself).  You can find the most current reports at:\n",
    "\n",
    "> https://www.gnosis.cx/stats/\n",
    "\n",
    "A snapshot of the reports current at the time of this writing are also copied to:\n",
    "\n",
    "> https://www.gnosis.cx/cleaning/stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image of the report page at the time of writing is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gnosis-traffic.png\" alt=\"Traffic report for gnosis.cx\" width=\"50%\"/>\n",
    "\n",
    "__Image: Traffic Report for gnosis.cx__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekly table shown is quite long since it goes back to February 2010.  The actual site is a decade older than that, but servers and logging databases were modified, losing older data.  There is also a rather large glitch of almost five years in the middle where traffic shows as zero.  The rather dramatic fall in traffic over the six weeks up to the snapshot reflects a change to using a CDN proxy for DNS and SSL (hence hiding traffic from the actual web host)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in this exercise is to write a tool to dynamically scrape the data made available in the various tables listing traffic sliced by different time increments and recurring periods (which day of week, which month of year, etc).  As part of this exercise, have your scripts generate less terrible graphs than the one shown in the screen picture (meaningless false perspective in a line graph offends good sensibility, and the apparent negative spike to negative traffic around the start of 2013 is merely inexplicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common need to scrape a website similar to these reports.  The pattern of having a regular and infrequently changed structure but updated contents on a daily basis, often reflects a data acquisition requirement.  A script like you will write in this exercise could run on a cronjob or under a similar mechanism, to maintain local copies and revisions of such rolling reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denouement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> They invaded the hexagons, showed credentials which were not always false, \n",
    "> leafed through a volume with displeasure and condemned whole shelves: their \n",
    "> hygienic, ascetic furor caused the senseless perdition of millions of books.<br/>\n",
    "> –Jorge Luis Borges (The Library of Babel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics**: Web Scraping; Portable Document Format; Image Formats; Binary Formats; Custom Text Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter contemplated data sources that you may not, in your first thought, think of as *data* per se.  Within web pages and PDF document, the intention is usually to present human readable content that only contains analyzable data as a secondary concern.  In the ideal situation, whoever produced those less structured documents will also provide structured versions of the same data; however, that ideal situation is only occasionally realized.  A few nicely written Free Software libaries let us do a reasonable job of extracting meaningful data from these source, albeit always in a way that is somewhat specific to the particular document, or at least to the family or revisions, of a particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are a very common interest in machine learning.  Drawing various conclusions about or characterizations of the content portrayed in images is a key application of deep neural networks, for example.  While those actual machine learning techniques are outside the scope of this particular book, this chapter introduced you to the basic APIs for acquiring an array/tensor representation of images, and performing some basic correction or normalization that will aid in those later machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are formats, as well, that while directly intended as means of recording and communicating data as such, are not widely used and tooling to read them directly may not be available to you.  The specific examples we present, for both binary and textual custom formats, are ones that library support exists for (less so for the text formats this chapter examines), but the general kinds of reasoning and approach to creating custom ingestion tools presented resemble those you will need to use when you encounter an antiquated, in-house, or merely idiosyncratic, format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chapter begins the next saga of this book.  These early chapters paid special attention to data formats you need to work with.  The next two chapters look at problems characteristic of data elements per se, not only their representation.  We begin by looking for anomalies in data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
